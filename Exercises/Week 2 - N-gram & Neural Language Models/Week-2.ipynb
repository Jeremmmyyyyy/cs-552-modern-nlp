{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3275ace2",
   "metadata": {},
   "source": [
    "# ðŸ“š  Exercise Session - Week 2\n",
    "\n",
    "Welcome to Week 2 exercise's session of CS552-Modern NLP!\n",
    "\n",
    "\n",
    "> **What will be covered:**\n",
    "1. [**TASK A:** N-gram Language Models](#ngram_lm)\n",
    "    - [Unigram Language Model](#unigram_lm)\n",
    "    - [Bi-gram Language Model](#bigram_lm)\n",
    "    - [Tri-gram Language Model](#trigram_lm)\n",
    "     \n",
    "2. [**TASK B:** Neural Language Models](#neural_lm)\n",
    "    - [Fixed-Window Neural Language Model](#fixed_window_lm)\n",
    "    - [RNN-based Language Model](#rnn_lm)\n",
    "\n",
    "> **By the end of the session you will be able to:**\n",
    "> - âœ…  Compute and interpret the perplexity of a language model \n",
    "> - âœ…  Implement N-gram language models for N=1,2,3\n",
    "> - âœ…  Implement, train, and evaluate a fixed window language model\n",
    "> - âœ…  Evaluate an RNN language model\n",
    "> - âœ…  Understand the advantages and disadvantages of each of the above models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the libraries if needed.\n",
    "# !pip install datasets\n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c0a7c",
   "metadata": {},
   "source": [
    "<a name=\"ngram_lm\"></a>\n",
    "## 1. Task A: N-gram Language Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c137c6",
   "metadata": {},
   "source": [
    "\n",
    "In this exercise, we will better understand the functioning of different types of (non-neural) language modeling, namely,  Unigram LM, Bi-gram LM, and Tri-gram LM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c0427",
   "metadata": {},
   "source": [
    "### 1.1 Unigram Language Model <a name=\"unigram_lm\"></a>\n",
    "In the simple Unigram language model, we pick/generate next token independent of the previous token. In other words, during the generation, we pick the tokens according to the token probability. Therefore, for an arbitrary sequence $x_1x_2~...x_n$, its respective probability becomes:\n",
    "$$p(x_1x_2~...x_n) = \\Pi_{i=1} ^n p(x_i)$$\n",
    "Let's use an unsupervised dataset (raw corpus) to evaluate this model's perplexity. We use Huggingface's `datasets` library to download needed datasets.\n",
    " \n",
    "\n",
    "Here we use the `Penn Treebank` dataset, featuring a million words of 1989 Wall Street Journal material. The rare words in this version are already replaced with `<unk>` token. The numbers are also replaced with a special token. This token replacement helps us to end up with a more reasonable vocabulary size to work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69afb9de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\modern_nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "ptb_dataset = load_dataset(\"ptb_text_only\", split=\"train\")\n",
    "\n",
    "# splitting dataset in train/test (to be later used for language model evaluation)\n",
    "ptb_dataset = ptb_dataset.train_test_split(test_size=0.2, seed=1)\n",
    "ptb_train, ptb_test = ptb_dataset['train'], ptb_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20589cd0",
   "metadata": {},
   "source": [
    "#### Let's have a look at a few samples of the training dataset (and also the structure of the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ceb684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': \"a former executive agreed that the departures do n't reflect major problems adding if you see any company that grows as fast as reebok did it is going to have people coming and going\"}\n",
      "\n",
      "{'sentence': 'with talk today of a second economic <unk> in west germany east germany no longer can content itself with being the economic star in a loser league'}\n",
      "\n",
      "{'sentence': 'transportation secretary sam skinner who earlier fueled the anti-takeover fires with his <unk> attacks on foreign investment in u.s. carriers now says the bill would further <unk> the jittery capital markets'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{ptb_train[0]}\\n\\n{ptb_train[1]}\\n\\n{ptb_train[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07207b13",
   "metadata": {},
   "source": [
    "During generation with a given language model, we often need to have a `<stop>` token in our vocabulary to terminate the generation of a given sentence/paragraph. In this dataset, every sample is a sentence, and the `<stop>` token should be added to the end of every sample (i.e., end of sentence).\n",
    "\n",
    "#### Create a new train/test dataset starting from `ptb_train` and `ptb_test` that has a `<stop>` at the end of each sentence. (Note: do not change the structure of the datasets objects, and just change the respective sentences as discussed).\n",
    "Hint: use the `.map()` functionality of the `datasets` package (read more [here](https://huggingface.co/docs/datasets/process#map]))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1b7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stop_token(input_sample: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sample: a dict representing a sample of the dataset. (look above for the dict struture)\n",
    "    output:\n",
    "        modified_sample: modified dict adding <stop> at the end of each sentence.\n",
    "    '''\n",
    "    modified_sample = input_sample.copy()\n",
    "    modified_sample['sentence'] = input_sample['sentence'] + \" <stop>\"\n",
    "    \n",
    "    return modified_sample\n",
    "    \n",
    "    \n",
    "ptb_train = ptb_train.map(add_stop_token)\n",
    "ptb_test = ptb_test.map(add_stop_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d338821",
   "metadata": {},
   "source": [
    "For both `ptb_train` and `ptb_test` datasets, filter out every sample that has less than 3 tokens. it will help remove very short sentences that are not very helpful for training/evaluating a langugage model.\n",
    "\n",
    "Hint: use `.filter()` functionality of the `datasets` package (read more [here](https://huggingface.co/docs/datasets/process#select-and-filter))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c0222f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33654/33654 [00:00<00:00, 75084.94 examples/s]\n",
      "Filter:   0%|          | 0/8414 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8414/8414 [00:00<00:00, 100136.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ptb_train = ptb_train.filter(lambda x : len(x['sentence'].split()) > 3)\n",
    "ptb_test = ptb_test.filter(lambda x : len(x['sentence'].split()) > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8249a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"a former executive agreed that the departures do n't reflect major problems adding if you see any company that grows as fast as reebok did it is going to have people coming and going <stop>\",\n",
       " 'with talk today of a second economic <unk> in west germany east germany no longer can content itself with being the economic star in a loser league <stop>',\n",
       " 'transportation secretary sam skinner who earlier fueled the anti-takeover fires with his <unk> attacks on foreign investment in u.s. carriers now says the bill would further <unk> the jittery capital markets <stop>',\n",
       " \"separately the company 's board adopted a proposal to <unk> its N shareholder rights plan further <unk> the company from takeover <stop>\",\n",
       " \"thomas p. <unk> chief financial officer would n't comment about the details of the negotiations <stop>\",\n",
       " 'before the recent <unk> in global financial markets b.a.t officials holders and analysts had expected a substantial part of the restructuring to be complete by the end of the first half <stop>',\n",
       " 'the latest period includes gains of $ N million from early retirement of debt and tax loss carry-forward <stop>',\n",
       " 'this city of more than N is known for <unk> <unk> and rich residents <stop>',\n",
       " 'some bankers are reporting more inquiries than usual about cds since friday <stop>',\n",
       " 'mr. <unk> said in the filing that he sold the stock to decrease his position in the <unk> ill. banking concern <stop>',\n",
       " \"he said the improvement was a result of cost savings achieved by consolidating manufacturing operations <unk> two sales organizations and focusing more carefully the company 's promotional activities <stop>\",\n",
       " 'it includes removing $ N million in good will from the books issuing $ N million in preferred stock and <unk> an exchange offer for $ N million in convertible bonds <stop>',\n",
       " '<unk> <unk> managing director of nomura investment trust management said that if the u.s. federal funds rate declines to around N N institutions would acquire a <unk> idea regarding the direction of the market and thus more <unk> participate in active buying <stop>',\n",
       " 'the initial <unk> was caught by cameras in downtown houston about N miles away <stop>',\n",
       " '<unk> <unk> N years old senior vice president marketing at <unk> entertainment inc. was named president of capitol records inc. a unit of this entertainment concern <stop>',\n",
       " 'the closely held supermarket chain named frank <unk> vice president and treasurer <stop>',\n",
       " 'earlier reports which revealed that as many as N companies were avoiding income tax legally have been credited with helping <unk> efforts to overhaul the tax code <stop>',\n",
       " \"if you think you have stress-related problems on the job there 's good news and bad news <stop>\",\n",
       " 'one sign that more soviet purchases are possible is that u.s. grain companies yesterday bought an unusually large amount of corn futures contracts <stop>',\n",
       " \"teddy the <unk> of max 's sons has made the most dramatic escape by becoming a professor of philosophy at an american university <stop>\",\n",
       " \"ibm though long a leader in the japanese mainframe business did n't introduce its first pc in japan until five years after nec did and that was n't compatible even with the u.s. ibm standard <stop>\",\n",
       " 'an n.v <unk> unit has created a computer system that processes video images N times faster than conventional systems <stop>',\n",
       " \"seeking to <unk> european concerns u.s. agriculture secretary <unk> <unk> said in washington that the new u.s. plan would n't put farmers out of business but would encourage them to grow what the markets desire instead of what the government wants <stop>\",\n",
       " \"these include among other parts each jetliner 's two major <unk> a pressure floor <unk> box fixed leading <unk> for the wings and an <unk> <unk> beam <stop>\",\n",
       " 'managers who retire dec. N will have an additional N N added to their monthly pension for as long as five years or age N whichever comes earlier <stop>',\n",
       " 'furukawa electric co. said it plans to increase production of computer memory devices on a large scale in the u.s. and japan <stop>',\n",
       " \"they did n't play the third game of the world series on tuesday night as scheduled and they did n't play it on wednesday or thursday either <stop>\",\n",
       " \"<unk> carter went further in a N campaign promise if i become president i 'll make every piece of information this country has about ufo <unk> available to the public and the scientists <stop>\",\n",
       " '<unk> order this is another timing order <stop>',\n",
       " \"u.s. farmers ' net income rose to a record $ N billion last year despite one of the worst <unk> ever <stop>\",\n",
       " 'the two developments leave the airline with several problems including an unsettled labor situation <stop>',\n",
       " \"according to an analyst cuba ca n't meet all its shipment commitments and has asked japan to accept a delay of shipments scheduled for later this year into early next year <stop>\",\n",
       " 'nashua strengthened its <unk> plan after announcing a dutch firm is seeking to buy up to N N of the new hampshire <unk> company <stop>',\n",
       " 'it would allow all defendants to introduce statistical evidence showing <unk> disproportionate application of the death penalty in the past <stop>',\n",
       " 'last year more than N traders on the cboe bought and sold N million contracts on N listed stocks N N of all u.s. listed options trading <stop>',\n",
       " 'crime was the reason that N N reported difficulty recruiting personnel and that N N said they were considering moving <stop>',\n",
       " 'now mr. bush will come to costa rica and <unk> nicaraguan <unk> daniel ortega eager for photo opportunities with the u.s. president <stop>',\n",
       " 'now when i play with <unk> the musicians often <unk> me on my <unk> <stop>',\n",
       " 'with no <unk> news or changes in the fundamentals to <unk> price moves technicians are wanting to sell this stuff said eric <unk> of edge trading corp <stop>',\n",
       " \"warner communications inc. is close to an agreement to back a new recorded music and music publishing company in partnership with irving azoff who resigned in september as head of mca inc. 's mca records unit <stop>\",\n",
       " 'the guide is young and he knows this business but he wants a different life after college such as working for ibm and wearing a <unk> <stop>',\n",
       " 'in the first nine months net was $ N million or $ N a share on sales of $ N billion <stop>',\n",
       " \"on <unk> relations it 's such a <unk> relationship going back into history <stop>\",\n",
       " 'with only small help from the government these start-up concerns are trying to compete with the <unk> of the japanese consumer electronics industry which enjoy considerable backing from the japanese government <stop>',\n",
       " '$ N million of floating-rate notes due november N paying six-month london interbank offered rate plus N point and priced at par via credit suisse first boston ltd <stop>',\n",
       " 'the company later <unk> and agreed to make $ N million in contributions to charities chosen by him <stop>',\n",
       " 'the decision to distribute <unk> at this time which could be used as weapons is under review said a u.n. spokesman <stop>',\n",
       " 'where does that first <unk> go <unk> <unk> <unk> paul <unk> <stop>',\n",
       " 'about N people typically travel over the golden gate bridge during commute hours <stop>',\n",
       " 'as a result mr. mengistu has been forced to transfer thousands of troops from <unk> just to hold the town thereby <unk> the loss of even more territory in <unk> only to keep the <unk> at bay <stop>',\n",
       " 'almost half N N reported suffering mental abuse from coaches and almost <unk> N N said they had been pressured to ignore injuries <stop>',\n",
       " 'and neither can your pilot <stop>',\n",
       " 'early yesterday bonds rose as investors rushed to buy treasury securities on the prospect that stocks would plummet in the aftermath of the massive california earthquake <stop>',\n",
       " 'eastern has disputed the claim but a federal district court an appeals court and now the arbitrator have all <unk> with the pilots <stop>',\n",
       " \"there 's either more to come or an extremely long period of <unk> <stop>\",\n",
       " 'when a large group of pilots once signed <unk> opposing <unk> and compensation changes he called a meeting in a company <unk> and dressed them down for challenging his authority <stop>',\n",
       " 'installation of satellite <unk> tvs and videocassette equipment will cost the company about $ N per school mr. whittle said <stop>',\n",
       " \"as mr. reagan 's running <unk> though mr. bush plunged <unk> into the anti-abortion position <unk> a constitutional amendment <unk> abortion <stop>\",\n",
       " 'the senate after <unk> section N repeal from its deficit-reduction bill still is expected to join the house in voting to kill the law which forces companies to provide comparable benefits to <unk> and executives alike <stop>',\n",
       " \"in our formula we do n't have any losers except the competition <stop>\",\n",
       " 'about N east germans marched in leipzig and thousands more staged protests in three other cities in a fresh challenge to the communist leadership to introduce democratic freedoms <stop>',\n",
       " \"the circuit breakers caused a <unk> shutdown in trading in standard & poor 's 500-stock index futures contract as the markets were falling <stop>\",\n",
       " 'it is expected to report next summer <stop>',\n",
       " 'an enormous <unk> has succeeded where the government has failed he has made speaking filipino respectable <stop>',\n",
       " 'mr. <unk> N a <unk> veteran at bearings has been president since N <stop>',\n",
       " \"according to a nationwide survey taken a year ago nearly a third of england 's church bells are no longer <unk> on <unk> because there is no one to ring them <stop>\",\n",
       " \"brown 's story <stop>\",\n",
       " \"northeast utilities ' plan proposes N N annual increases <stop>\",\n",
       " 'moreover they note those who manage to pay their own way often do so only by selling their homes using up life savings or drawing heavily on children and other <unk> <stop>',\n",
       " 'the problem with this administration i think is that by design it has greatly diminished both in a physical sense and in a procedural sense the role of the nsc <stop>',\n",
       " \"they are n't showing james madison taking a <unk> or lighting up says laurence tribe a professor of constitutional law at harvard university <stop>\",\n",
       " \"one person familiar with ual said the unsettled labor situation and the uncertain world-wide financial markets contributed to the board 's decision to avoid rushing around selling the company at a bargain price particularly since it accepted a $ 300-a-share offer just last month <stop>\",\n",
       " 'however companies with few catastrophe losses this year and already big buyers of reinsurance are likely to see their rates remain flat or perhaps even decline slightly <stop>',\n",
       " 'so what does george bush really believe <stop>',\n",
       " 'one of their <unk> has <unk> a new <unk> of creative socialism <stop>',\n",
       " \"the merchandise trade deficit widened in august to $ N billion the commerce department reported a sharp deterioration from july 's $ N billion and the largest deficit of any month this year <stop>\",\n",
       " 'a series of recent acquisitions made it the dominant magazine publisher in quebec <stop>',\n",
       " 'a competing version of epo is being developed by genetics institute inc. in cambridge mass <stop>',\n",
       " \"arbitrage does n't cause volatility it <unk> to it <stop>\",\n",
       " 'today so-called mountain bikes account for two-thirds of the $ N billion spent annually on all <unk> in the u.s. <stop>',\n",
       " 'the voice of america is a government agency that <unk> news and views some might say propaganda in N <unk> to N million <unk> around the world <stop>',\n",
       " 'the plan was complete except for finishing <unk> and there was talk that it would be unveiled as early as yesterday <stop>',\n",
       " \"finnair finland 's state-owned airline joined the wave of global airline alliances and signed a <unk> cooperation agreement with archrival <unk> airlines system <stop>\",\n",
       " 'pending an appeal by the new zealand team led by michael <unk> the <unk> for the next cup challenge are scheduled to be held in <unk> in san diego <stop>',\n",
       " 'sterling had completed a five-year contract for nasa but lost its bid for renewal <stop>',\n",
       " 'also withdrawals exceeded deposits by $ N billion in the month <stop>',\n",
       " 'the big board declined to name the wall street firms involved in the activity friday and monday or the type of strategies used <stop>',\n",
       " 'he hit the <unk> three years ago on the advice of his doctor <stop>',\n",
       " \"that 's where the two scripts would <unk> <stop>\",\n",
       " \"nelson holdings international ltd. shareholders approved a <unk> consolidation of the company 's common stock at a special meeting <stop>\",\n",
       " 'the loss is a setback to ibm which pointed to the kodak contract as an example of its success in systems integration <stop>',\n",
       " 'traffic is certainly a concern as is pollution water and an adequate labor market says frank <unk> executive director of the las vegas convention and visitors bureau <stop>',\n",
       " \"profits improved across hess 's businesses <stop>\",\n",
       " \"it did n't provide details of <unk> costs <stop>\",\n",
       " 'nevertheless an anc rally by any other name is still an anc rally <stop>',\n",
       " \"the soviet state bank announced a N N devaluation of the ruble against the dollar for private transactions in an apparent attempt to curb the nation 's rapidly growing black market for hard currency <stop>\",\n",
       " 'the primary <unk> thing is that people are frightened says martin <unk> a new york money manager <stop>',\n",
       " '<unk> mrs. ward says the school was having trouble serving in harmony its two <unk> and evenly split student groups a <unk> white elite from old <unk> neighborhoods and blacks many of them poor from <unk> inner city neighborhoods <stop>',\n",
       " \"asked about the consultants ' reports an eastern spokeswoman said we totally disagree <stop>\",\n",
       " 'exxon corp. said its third-quarter earnings slipped N N as profits from two of its three major businesses sagged <stop>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptb_train['sentence'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733f77b",
   "metadata": {},
   "source": [
    "#### What are the 10 most frequent tokens in this dataset? Can you spot the token used to replace the numbers in this dataset? How are rare tokens replaced in this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eaec09",
   "metadata": {},
   "source": [
    "#### Now let's create a dictionary of the word probabilites (in the format of `{word: Prob(word)}`in the following function. We will use these probabilities to estimate sequence probabilities for a given sequence, as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e015a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 40612,\n",
       "         '<unk>': 35814,\n",
       "         '<stop>': 33306,\n",
       "         'N': 25940,\n",
       "         'of': 19459,\n",
       "         'to': 18896,\n",
       "         'a': 16899,\n",
       "         'in': 14472,\n",
       "         'and': 14013,\n",
       "         \"'s\": 7850,\n",
       "         'for': 7108,\n",
       "         'that': 7100,\n",
       "         '$': 5954,\n",
       "         'is': 5923,\n",
       "         'it': 4866,\n",
       "         'said': 4840,\n",
       "         'on': 4493,\n",
       "         'at': 3946,\n",
       "         'by': 3939,\n",
       "         'as': 3860,\n",
       "         'from': 3818,\n",
       "         'with': 3678,\n",
       "         'million': 3655,\n",
       "         'mr.': 3468,\n",
       "         'was': 3245,\n",
       "         'be': 3122,\n",
       "         'are': 3111,\n",
       "         'its': 3089,\n",
       "         'he': 2897,\n",
       "         'but': 2841,\n",
       "         'has': 2818,\n",
       "         'an': 2787,\n",
       "         \"n't\": 2694,\n",
       "         'will': 2582,\n",
       "         'have': 2557,\n",
       "         'new': 2228,\n",
       "         'company': 2175,\n",
       "         'or': 2141,\n",
       "         'they': 2074,\n",
       "         'this': 1953,\n",
       "         'year': 1908,\n",
       "         'which': 1872,\n",
       "         'would': 1840,\n",
       "         'about': 1732,\n",
       "         'says': 1658,\n",
       "         'were': 1632,\n",
       "         'more': 1626,\n",
       "         'market': 1587,\n",
       "         'billion': 1505,\n",
       "         'his': 1490,\n",
       "         'had': 1466,\n",
       "         'their': 1444,\n",
       "         'one': 1413,\n",
       "         'up': 1408,\n",
       "         'u.s.': 1391,\n",
       "         'than': 1366,\n",
       "         'some': 1360,\n",
       "         'who': 1359,\n",
       "         'been': 1344,\n",
       "         'also': 1290,\n",
       "         'stock': 1280,\n",
       "         'other': 1267,\n",
       "         'share': 1203,\n",
       "         'not': 1154,\n",
       "         'we': 1094,\n",
       "         'if': 1029,\n",
       "         'when': 1018,\n",
       "         'corp.': 1015,\n",
       "         'i': 1003,\n",
       "         'years': 1002,\n",
       "         'all': 997,\n",
       "         'president': 994,\n",
       "         'last': 991,\n",
       "         'shares': 989,\n",
       "         'first': 953,\n",
       "         'trading': 952,\n",
       "         'two': 933,\n",
       "         'inc.': 923,\n",
       "         'because': 913,\n",
       "         'could': 896,\n",
       "         'sales': 889,\n",
       "         'after': 880,\n",
       "         'there': 873,\n",
       "         '&': 859,\n",
       "         'business': 852,\n",
       "         'out': 847,\n",
       "         'do': 808,\n",
       "         'only': 800,\n",
       "         'can': 795,\n",
       "         'such': 788,\n",
       "         'co.': 786,\n",
       "         'may': 780,\n",
       "         'york': 773,\n",
       "         'most': 766,\n",
       "         'over': 765,\n",
       "         'into': 763,\n",
       "         'group': 754,\n",
       "         'federal': 731,\n",
       "         'many': 727,\n",
       "         'government': 721,\n",
       "         'now': 699,\n",
       "         'no': 695,\n",
       "         'time': 681,\n",
       "         'bank': 670,\n",
       "         'companies': 667,\n",
       "         'any': 660,\n",
       "         'so': 660,\n",
       "         'people': 653,\n",
       "         'cents': 650,\n",
       "         'quarter': 636,\n",
       "         'you': 629,\n",
       "         'exchange': 627,\n",
       "         'what': 625,\n",
       "         'prices': 624,\n",
       "         'price': 613,\n",
       "         'even': 609,\n",
       "         'say': 605,\n",
       "         'while': 602,\n",
       "         'down': 600,\n",
       "         'rose': 593,\n",
       "         'much': 582,\n",
       "         'investors': 579,\n",
       "         'yesterday': 576,\n",
       "         'under': 569,\n",
       "         'big': 569,\n",
       "         'week': 568,\n",
       "         'securities': 559,\n",
       "         'them': 547,\n",
       "         'months': 546,\n",
       "         'next': 526,\n",
       "         \"'\": 523,\n",
       "         'major': 520,\n",
       "         'bonds': 520,\n",
       "         'make': 520,\n",
       "         'earnings': 520,\n",
       "         'interest': 520,\n",
       "         'earlier': 519,\n",
       "         'three': 519,\n",
       "         'american': 513,\n",
       "         'state': 513,\n",
       "         'financial': 512,\n",
       "         'net': 509,\n",
       "         'investment': 503,\n",
       "         'still': 502,\n",
       "         'chairman': 498,\n",
       "         'these': 497,\n",
       "         'since': 493,\n",
       "         'chief': 492,\n",
       "         'those': 489,\n",
       "         'did': 487,\n",
       "         'board': 484,\n",
       "         'before': 479,\n",
       "         'through': 479,\n",
       "         'program': 477,\n",
       "         'industry': 475,\n",
       "         'executive': 472,\n",
       "         'made': 470,\n",
       "         'stocks': 470,\n",
       "         'just': 466,\n",
       "         'officials': 464,\n",
       "         'rate': 462,\n",
       "         'national': 459,\n",
       "         'like': 446,\n",
       "         'money': 445,\n",
       "         'month': 441,\n",
       "         'analysts': 439,\n",
       "         'house': 431,\n",
       "         'rates': 427,\n",
       "         'she': 425,\n",
       "         'off': 422,\n",
       "         'profit': 421,\n",
       "         'expected': 420,\n",
       "         'against': 420,\n",
       "         'plan': 417,\n",
       "         'both': 417,\n",
       "         'does': 415,\n",
       "         'days': 410,\n",
       "         'unit': 408,\n",
       "         'between': 405,\n",
       "         'capital': 404,\n",
       "         'income': 399,\n",
       "         'buy': 393,\n",
       "         'recent': 392,\n",
       "         'get': 390,\n",
       "         'markets': 389,\n",
       "         'japanese': 387,\n",
       "         'sell': 387,\n",
       "         'issue': 384,\n",
       "         'firm': 383,\n",
       "         'well': 382,\n",
       "         'during': 381,\n",
       "         'ago': 378,\n",
       "         'average': 378,\n",
       "         'revenue': 377,\n",
       "         'own': 374,\n",
       "         'should': 373,\n",
       "         'back': 372,\n",
       "         'international': 372,\n",
       "         'general': 370,\n",
       "         'products': 370,\n",
       "         'her': 369,\n",
       "         'another': 368,\n",
       "         'part': 365,\n",
       "         'fell': 364,\n",
       "         'funds': 363,\n",
       "         'being': 362,\n",
       "         'offer': 361,\n",
       "         'higher': 361,\n",
       "         'each': 360,\n",
       "         'take': 360,\n",
       "         'debt': 359,\n",
       "         'japan': 357,\n",
       "         'among': 357,\n",
       "         'index': 357,\n",
       "         'including': 355,\n",
       "         'court': 350,\n",
       "         'tax': 349,\n",
       "         'world': 348,\n",
       "         'work': 348,\n",
       "         'past': 347,\n",
       "         'according': 343,\n",
       "         'sale': 343,\n",
       "         'operations': 342,\n",
       "         'report': 342,\n",
       "         'computer': 341,\n",
       "         'trade': 340,\n",
       "         'reported': 337,\n",
       "         'our': 337,\n",
       "         'then': 335,\n",
       "         'economic': 330,\n",
       "         'vice': 330,\n",
       "         'however': 329,\n",
       "         'plans': 328,\n",
       "         'how': 328,\n",
       "         'high': 326,\n",
       "         'department': 325,\n",
       "         'sold': 324,\n",
       "         'yield': 322,\n",
       "         'end': 321,\n",
       "         'less': 321,\n",
       "         'several': 320,\n",
       "         'yen': 320,\n",
       "         'way': 319,\n",
       "         'banks': 319,\n",
       "         'closed': 318,\n",
       "         'lower': 318,\n",
       "         'foreign': 317,\n",
       "         'growth': 317,\n",
       "         'insurance': 317,\n",
       "         'increase': 316,\n",
       "         'pay': 314,\n",
       "         'common': 308,\n",
       "         'very': 308,\n",
       "         'bid': 305,\n",
       "         'cash': 305,\n",
       "         'where': 301,\n",
       "         'issues': 301,\n",
       "         'day': 301,\n",
       "         'system': 300,\n",
       "         'current': 300,\n",
       "         'bill': 299,\n",
       "         'law': 299,\n",
       "         'five': 298,\n",
       "         'bush': 298,\n",
       "         'used': 298,\n",
       "         'management': 297,\n",
       "         'due': 296,\n",
       "         'early': 295,\n",
       "         'loss': 291,\n",
       "         'traders': 291,\n",
       "         'officer': 290,\n",
       "         'oil': 290,\n",
       "         'use': 290,\n",
       "         'third': 289,\n",
       "         'few': 289,\n",
       "         \"'re\": 288,\n",
       "         'friday': 287,\n",
       "         'good': 287,\n",
       "         'added': 287,\n",
       "         'california': 286,\n",
       "         'public': 285,\n",
       "         'costs': 285,\n",
       "         'office': 285,\n",
       "         'already': 284,\n",
       "         'futures': 283,\n",
       "         'bond': 283,\n",
       "         'city': 282,\n",
       "         'might': 281,\n",
       "         'assets': 281,\n",
       "         'san': 280,\n",
       "         'spokesman': 276,\n",
       "         'director': 275,\n",
       "         'oct.': 275,\n",
       "         'late': 274,\n",
       "         'least': 274,\n",
       "         'based': 274,\n",
       "         'though': 273,\n",
       "         'september': 273,\n",
       "         'small': 273,\n",
       "         'far': 272,\n",
       "         'news': 270,\n",
       "         'congress': 268,\n",
       "         'treasury': 267,\n",
       "         'agency': 267,\n",
       "         'number': 267,\n",
       "         'concern': 266,\n",
       "         'too': 266,\n",
       "         'british': 266,\n",
       "         'same': 265,\n",
       "         'going': 264,\n",
       "         'union': 264,\n",
       "         'street': 263,\n",
       "         'real': 263,\n",
       "         'him': 262,\n",
       "         'operating': 262,\n",
       "         'think': 261,\n",
       "         'research': 261,\n",
       "         'ended': 259,\n",
       "         'home': 259,\n",
       "         'until': 258,\n",
       "         'called': 258,\n",
       "         'fund': 258,\n",
       "         'today': 257,\n",
       "         'close': 257,\n",
       "         'contract': 257,\n",
       "         'case': 257,\n",
       "         'stake': 257,\n",
       "         'second': 256,\n",
       "         'wall': 255,\n",
       "         'little': 254,\n",
       "         'move': 253,\n",
       "         'dollar': 252,\n",
       "         'services': 252,\n",
       "         'former': 249,\n",
       "         'help': 249,\n",
       "         'value': 249,\n",
       "         'control': 248,\n",
       "         'analyst': 246,\n",
       "         'administration': 245,\n",
       "         'maker': 244,\n",
       "         'period': 243,\n",
       "         'put': 243,\n",
       "         'point': 243,\n",
       "         'third-quarter': 243,\n",
       "         'power': 243,\n",
       "         'credit': 242,\n",
       "         'problems': 241,\n",
       "         'results': 241,\n",
       "         'agreement': 240,\n",
       "         'offering': 240,\n",
       "         'corporate': 240,\n",
       "         'want': 239,\n",
       "         'service': 239,\n",
       "         'agreed': 238,\n",
       "         'cost': 238,\n",
       "         'economy': 238,\n",
       "         'six': 237,\n",
       "         'production': 234,\n",
       "         'four': 234,\n",
       "         'buying': 232,\n",
       "         'long': 232,\n",
       "         'firms': 232,\n",
       "         'here': 232,\n",
       "         'country': 231,\n",
       "         'life': 231,\n",
       "         'annual': 231,\n",
       "         'points': 231,\n",
       "         'recently': 231,\n",
       "         'likely': 230,\n",
       "         'without': 230,\n",
       "         'soviet': 229,\n",
       "         'committee': 229,\n",
       "         'total': 227,\n",
       "         'around': 226,\n",
       "         'although': 226,\n",
       "         'cut': 226,\n",
       "         'see': 225,\n",
       "         'whether': 225,\n",
       "         'volume': 224,\n",
       "         'compared': 224,\n",
       "         'loans': 224,\n",
       "         'half': 223,\n",
       "         'policy': 223,\n",
       "         'increased': 223,\n",
       "         'august': 221,\n",
       "         'old': 220,\n",
       "         'west': 219,\n",
       "         'large': 219,\n",
       "         'set': 219,\n",
       "         'yet': 218,\n",
       "         'continue': 218,\n",
       "         'losses': 215,\n",
       "         'john': 215,\n",
       "         'right': 214,\n",
       "         'further': 213,\n",
       "         'notes': 213,\n",
       "         'go': 213,\n",
       "         'strong': 212,\n",
       "         'political': 212,\n",
       "         'declined': 211,\n",
       "         'must': 211,\n",
       "         'takeover': 210,\n",
       "         'result': 210,\n",
       "         'selling': 210,\n",
       "         'making': 210,\n",
       "         'my': 209,\n",
       "         'monday': 209,\n",
       "         'francisco': 208,\n",
       "         'plant': 208,\n",
       "         'judge': 208,\n",
       "         'announced': 207,\n",
       "         'ual': 205,\n",
       "         'gain': 205,\n",
       "         'times': 204,\n",
       "         'largest': 204,\n",
       "         'paper': 204,\n",
       "         'earthquake': 203,\n",
       "         'expects': 203,\n",
       "         'businesses': 202,\n",
       "         'support': 202,\n",
       "         'held': 201,\n",
       "         'wo': 200,\n",
       "         'come': 199,\n",
       "         'level': 199,\n",
       "         'systems': 198,\n",
       "         'london': 198,\n",
       "         'problem': 198,\n",
       "         'demand': 198,\n",
       "         'data': 198,\n",
       "         'weeks': 197,\n",
       "         'area': 197,\n",
       "         'record': 196,\n",
       "         'official': 196,\n",
       "         'nov.': 196,\n",
       "         'composite': 195,\n",
       "         'become': 194,\n",
       "         'inc': 194,\n",
       "         'workers': 194,\n",
       "         'members': 193,\n",
       "         'fiscal': 193,\n",
       "         'industrial': 193,\n",
       "         'trust': 192,\n",
       "         'priced': 192,\n",
       "         'holding': 192,\n",
       "         'south': 192,\n",
       "         'certain': 190,\n",
       "         'ford': 190,\n",
       "         'association': 190,\n",
       "         'orders': 189,\n",
       "         'development': 188,\n",
       "         'give': 188,\n",
       "         'estimated': 187,\n",
       "         'series': 186,\n",
       "         'white': 186,\n",
       "         'drop': 186,\n",
       "         'decline': 185,\n",
       "         'currently': 185,\n",
       "         'took': 185,\n",
       "         'estate': 185,\n",
       "         'air': 185,\n",
       "         'employees': 185,\n",
       "         'executives': 184,\n",
       "         'tuesday': 183,\n",
       "         'future': 183,\n",
       "         'return': 183,\n",
       "         'senior': 182,\n",
       "         'health': 182,\n",
       "         'proposal': 181,\n",
       "         'despite': 181,\n",
       "         'others': 181,\n",
       "         'change': 181,\n",
       "         'ms.': 181,\n",
       "         'meeting': 180,\n",
       "         'commission': 180,\n",
       "         'building': 180,\n",
       "         'loan': 180,\n",
       "         'know': 180,\n",
       "         'once': 179,\n",
       "         'senate': 179,\n",
       "         'nearly': 179,\n",
       "         'deal': 179,\n",
       "         'chicago': 179,\n",
       "         'position': 178,\n",
       "         'rise': 178,\n",
       "         'comment': 177,\n",
       "         'later': 177,\n",
       "         'example': 177,\n",
       "         'show': 177,\n",
       "         'jones': 177,\n",
       "         'investor': 177,\n",
       "         'latest': 176,\n",
       "         'possible': 176,\n",
       "         'received': 176,\n",
       "         'damage': 176,\n",
       "         'dow': 176,\n",
       "         'need': 176,\n",
       "         'proposed': 175,\n",
       "         'acquisition': 175,\n",
       "         'often': 174,\n",
       "         'almost': 174,\n",
       "         'your': 174,\n",
       "         'robert': 174,\n",
       "         'filed': 174,\n",
       "         'drug': 174,\n",
       "         'paid': 174,\n",
       "         'junk': 174,\n",
       "         'washington': 173,\n",
       "         'offered': 172,\n",
       "         'charge': 172,\n",
       "         'line': 172,\n",
       "         'addition': 172,\n",
       "         'defense': 172,\n",
       "         'amount': 171,\n",
       "         'order': 171,\n",
       "         'nation': 171,\n",
       "         'product': 171,\n",
       "         'commercial': 171,\n",
       "         'east': 170,\n",
       "         'better': 170,\n",
       "         'force': 170,\n",
       "         'top': 170,\n",
       "         'purchase': 170,\n",
       "         'spending': 169,\n",
       "         'previous': 169,\n",
       "         'division': 167,\n",
       "         'dropped': 167,\n",
       "         'rights': 166,\n",
       "         'terms': 166,\n",
       "         'texas': 166,\n",
       "         'jaguar': 166,\n",
       "         'october': 165,\n",
       "         'expect': 165,\n",
       "         'within': 164,\n",
       "         'named': 163,\n",
       "         'outstanding': 163,\n",
       "         'trying': 162,\n",
       "         'changes': 161,\n",
       "         'nine': 161,\n",
       "         'began': 161,\n",
       "         'decision': 160,\n",
       "         'told': 160,\n",
       "         'keep': 159,\n",
       "         'technology': 159,\n",
       "         'industries': 159,\n",
       "         'information': 158,\n",
       "         'fed': 158,\n",
       "         'customers': 157,\n",
       "         'america': 156,\n",
       "         'finance': 156,\n",
       "         'us': 156,\n",
       "         'enough': 156,\n",
       "         'include': 155,\n",
       "         'managers': 155,\n",
       "         'came': 155,\n",
       "         'found': 155,\n",
       "         'budget': 155,\n",
       "         'banking': 154,\n",
       "         'every': 154,\n",
       "         'equipment': 154,\n",
       "         'shareholders': 154,\n",
       "         'computers': 154,\n",
       "         'provide': 153,\n",
       "         'james': 153,\n",
       "         'private': 153,\n",
       "         'auto': 153,\n",
       "         'gains': 152,\n",
       "         'again': 152,\n",
       "         'programs': 152,\n",
       "         'got': 152,\n",
       "         'action': 152,\n",
       "         'states': 151,\n",
       "         'never': 151,\n",
       "         'mortgage': 151,\n",
       "         'car': 150,\n",
       "         'ca': 149,\n",
       "         'warner': 149,\n",
       "         'lot': 149,\n",
       "         'transaction': 149,\n",
       "         'july': 148,\n",
       "         'following': 148,\n",
       "         'financing': 148,\n",
       "         'europe': 148,\n",
       "         'instead': 146,\n",
       "         'believe': 146,\n",
       "         'charges': 146,\n",
       "         'units': 146,\n",
       "         'makes': 146,\n",
       "         'tokyo': 146,\n",
       "         'best': 146,\n",
       "         'additional': 145,\n",
       "         'los': 145,\n",
       "         'run': 145,\n",
       "         'equity': 145,\n",
       "         'inflation': 144,\n",
       "         'local': 144,\n",
       "         'below': 144,\n",
       "         'gas': 144,\n",
       "         'buy-out': 144,\n",
       "         'european': 143,\n",
       "         'ltd.': 143,\n",
       "         'food': 143,\n",
       "         'things': 143,\n",
       "         'united': 143,\n",
       "         'able': 143,\n",
       "         'restructuring': 142,\n",
       "         'asked': 142,\n",
       "         'fall': 142,\n",
       "         'dollars': 142,\n",
       "         'march': 142,\n",
       "         \"'ve\": 142,\n",
       "         'consumer': 141,\n",
       "         'mrs.': 141,\n",
       "         'great': 141,\n",
       "         'low': 141,\n",
       "         'important': 141,\n",
       "         'legal': 141,\n",
       "         'sept.': 141,\n",
       "         'university': 140,\n",
       "         'corp': 140,\n",
       "         'steel': 140,\n",
       "         'above': 140,\n",
       "         'security': 140,\n",
       "         'family': 140,\n",
       "         'place': 140,\n",
       "         'away': 139,\n",
       "         'available': 139,\n",
       "         'options': 139,\n",
       "         'potential': 139,\n",
       "         'china': 139,\n",
       "         'continued': 139,\n",
       "         'pacific': 138,\n",
       "         '#': 138,\n",
       "         'bills': 138,\n",
       "         'construction': 138,\n",
       "         'head': 137,\n",
       "         'boston': 137,\n",
       "         'lost': 137,\n",
       "         'special': 137,\n",
       "         'suit': 137,\n",
       "         'raise': 137,\n",
       "         'holders': 136,\n",
       "         'marketing': 136,\n",
       "         'contracts': 136,\n",
       "         'led': 136,\n",
       "         'risk': 136,\n",
       "         'effect': 136,\n",
       "         'western': 136,\n",
       "         'co': 136,\n",
       "         'june': 135,\n",
       "         'david': 135,\n",
       "         'taken': 134,\n",
       "         'manager': 134,\n",
       "         'whose': 134,\n",
       "         'claims': 134,\n",
       "         'stores': 134,\n",
       "         'open': 134,\n",
       "         'subsidiary': 134,\n",
       "         'advertising': 133,\n",
       "         'personal': 133,\n",
       "         'included': 133,\n",
       "         'full': 133,\n",
       "         'approval': 132,\n",
       "         'figures': 132,\n",
       "         'posted': 132,\n",
       "         'ibm': 131,\n",
       "         'working': 131,\n",
       "         'via': 131,\n",
       "         'account': 131,\n",
       "         'reports': 130,\n",
       "         'either': 130,\n",
       "         'effort': 130,\n",
       "         'tv': 130,\n",
       "         'gold': 130,\n",
       "         'face': 130,\n",
       "         'noted': 129,\n",
       "         'statement': 129,\n",
       "         'lawyers': 129,\n",
       "         'meanwhile': 129,\n",
       "         'rather': 128,\n",
       "         'fact': 128,\n",
       "         'd.': 128,\n",
       "         'similar': 128,\n",
       "         'increases': 127,\n",
       "         'institute': 127,\n",
       "         'countries': 127,\n",
       "         'cars': 127,\n",
       "         'school': 126,\n",
       "         'soon': 126,\n",
       "         'efforts': 125,\n",
       "         'airlines': 125,\n",
       "         'network': 125,\n",
       "         'individual': 125,\n",
       "         'profits': 124,\n",
       "         'long-term': 124,\n",
       "         'left': 124,\n",
       "         'cases': 124,\n",
       "         'find': 124,\n",
       "         'directors': 124,\n",
       "         'secretary': 123,\n",
       "         'groups': 123,\n",
       "         'reserve': 123,\n",
       "         'domestic': 123,\n",
       "         'angeles': 123,\n",
       "         'probably': 123,\n",
       "         'known': 122,\n",
       "         'bought': 122,\n",
       "         'percentage': 122,\n",
       "         'germany': 121,\n",
       "         'portfolio': 121,\n",
       "         'parent': 121,\n",
       "         'telephone': 121,\n",
       "         'gained': 121,\n",
       "         'getting': 121,\n",
       "         'center': 121,\n",
       "         'slightly': 120,\n",
       "         'north': 120,\n",
       "         'something': 120,\n",
       "         'talks': 120,\n",
       "         'biggest': 120,\n",
       "         'reduce': 120,\n",
       "         'remain': 119,\n",
       "         'along': 119,\n",
       "         'looking': 119,\n",
       "         'look': 119,\n",
       "         'dealers': 119,\n",
       "         'machines': 119,\n",
       "         'failed': 118,\n",
       "         'strategy': 118,\n",
       "         'reached': 118,\n",
       "         'clients': 118,\n",
       "         'view': 118,\n",
       "         'coming': 117,\n",
       "         'magazine': 117,\n",
       "         'approved': 117,\n",
       "         'ad': 117,\n",
       "         'previously': 117,\n",
       "         'party': 117,\n",
       "         'clear': 117,\n",
       "         'standard': 116,\n",
       "         'labor': 116,\n",
       "         'bay': 116,\n",
       "         'hard': 116,\n",
       "         'limited': 116,\n",
       "         'why': 116,\n",
       "         'process': 116,\n",
       "         'chemical': 116,\n",
       "         'given': 116,\n",
       "         'calls': 116,\n",
       "         'helped': 116,\n",
       "         'attorney': 116,\n",
       "         'marks': 116,\n",
       "         'joint': 116,\n",
       "         'savings': 115,\n",
       "         'manufacturing': 115,\n",
       "         'using': 115,\n",
       "         'role': 115,\n",
       "         'having': 115,\n",
       "         'saying': 115,\n",
       "         'canadian': 115,\n",
       "         'payments': 115,\n",
       "         'britain': 115,\n",
       "         'itself': 114,\n",
       "         'communications': 114,\n",
       "         'different': 114,\n",
       "         'question': 114,\n",
       "         'performance': 114,\n",
       "         'goods': 114,\n",
       "         'free': 114,\n",
       "         'black': 113,\n",
       "         'act': 113,\n",
       "         'calif.': 113,\n",
       "         'levels': 113,\n",
       "         'makers': 113,\n",
       "         'merrill': 113,\n",
       "         'estimates': 113,\n",
       "         'went': 112,\n",
       "         'eastern': 112,\n",
       "         'team': 112,\n",
       "         'activity': 112,\n",
       "         'medical': 112,\n",
       "         'owns': 112,\n",
       "         'hong': 112,\n",
       "         'short': 112,\n",
       "         'german': 112,\n",
       "         'job': 111,\n",
       "         'property': 111,\n",
       "         'investments': 111,\n",
       "         'canada': 111,\n",
       "         'lynch': 111,\n",
       "         'institutions': 110,\n",
       "         'growing': 110,\n",
       "         'aid': 110,\n",
       "         'military': 110,\n",
       "         'year-earlier': 110,\n",
       "         'dividend': 110,\n",
       "         'buyers': 109,\n",
       "         'name': 109,\n",
       "         'lead': 109,\n",
       "         'huge': 109,\n",
       "         'plants': 109,\n",
       "         'range': 109,\n",
       "         'raised': 109,\n",
       "         'remains': 109,\n",
       "         'television': 108,\n",
       "         'interests': 108,\n",
       "         'me': 107,\n",
       "         'particularly': 107,\n",
       "         'michael': 107,\n",
       "         'completed': 107,\n",
       "         'leaders': 107,\n",
       "         'vote': 107,\n",
       "         'legislation': 107,\n",
       "         'toward': 107,\n",
       "         'transportation': 106,\n",
       "         'includes': 106,\n",
       "         'seeking': 106,\n",
       "         'concerns': 106,\n",
       "         'pressure': 106,\n",
       "         'ever': 106,\n",
       "         'district': 106,\n",
       "         'taking': 106,\n",
       "         'especially': 106,\n",
       "         'started': 106,\n",
       "         'call': 106,\n",
       "         's&p': 106,\n",
       "         'seems': 106,\n",
       "         'william': 106,\n",
       "         'kong': 106,\n",
       "         'leading': 105,\n",
       "         'scheduled': 105,\n",
       "         'airline': 105,\n",
       "         'gm': 105,\n",
       "         'heavy': 105,\n",
       "         'project': 105,\n",
       "         'french': 105,\n",
       "         'central': 105,\n",
       "         'lines': 105,\n",
       "         'fees': 105,\n",
       "         'staff': 105,\n",
       "         'allow': 104,\n",
       "         'abortion': 104,\n",
       "         'deficit': 104,\n",
       "         'involved': 104,\n",
       "         'columbia': 104,\n",
       "         \"'m\": 104,\n",
       "         'outside': 104,\n",
       "         'acquired': 104,\n",
       "         'congressional': 103,\n",
       "         'richard': 103,\n",
       "         'always': 103,\n",
       "         'issued': 103,\n",
       "         'meet': 102,\n",
       "         'hours': 102,\n",
       "         'really': 102,\n",
       "         'energy': 102,\n",
       "         'producers': 102,\n",
       "         'independent': 102,\n",
       "         'care': 102,\n",
       "         'begin': 102,\n",
       "         'venture': 102,\n",
       "         'rep.': 102,\n",
       "         'campaign': 101,\n",
       "         'hold': 101,\n",
       "         'currency': 101,\n",
       "         'april': 101,\n",
       "         'drexel': 101,\n",
       "         'themselves': 101,\n",
       "         'basis': 101,\n",
       "         'wants': 100,\n",
       "         'needed': 100,\n",
       "         'conference': 100,\n",
       "         'means': 100,\n",
       "         'morgan': 100,\n",
       "         'impact': 100,\n",
       "         'estimate': 100,\n",
       "         'acquire': 99,\n",
       "         \"'ll\": 99,\n",
       "         'volatility': 99,\n",
       "         'adds': 99,\n",
       "         'j.': 99,\n",
       "         'daily': 99,\n",
       "         'plc': 99,\n",
       "         'study': 99,\n",
       "         'consider': 99,\n",
       "         'bad': 98,\n",
       "         'dec.': 98,\n",
       "         'wednesday': 98,\n",
       "         'hit': 98,\n",
       "         'reduced': 98,\n",
       "         'try': 98,\n",
       "         'related': 98,\n",
       "         'turn': 98,\n",
       "         'man': 98,\n",
       "         'start': 98,\n",
       "         'significant': 98,\n",
       "         'leader': 97,\n",
       "         'competition': 97,\n",
       "         'taxes': 97,\n",
       "         'test': 97,\n",
       "         'accounts': 97,\n",
       "         'base': 97,\n",
       "         'quickly': 97,\n",
       "         'key': 97,\n",
       "         'earned': 97,\n",
       "         'doing': 97,\n",
       "         'projects': 97,\n",
       "         'reserves': 97,\n",
       "         'parts': 96,\n",
       "         'build': 96,\n",
       "         'prime': 96,\n",
       "         'partners': 96,\n",
       "         'community': 96,\n",
       "         'brokerage': 96,\n",
       "         'reason': 95,\n",
       "         'young': 95,\n",
       "         'rally': 95,\n",
       "         'considered': 95,\n",
       "         'press': 95,\n",
       "         'traded': 95,\n",
       "         'women': 95,\n",
       "         'measure': 95,\n",
       "         'beginning': 95,\n",
       "         'supply': 95,\n",
       "         'course': 95,\n",
       "         'produce': 95,\n",
       "         'journal': 95,\n",
       "         'convertible': 94,\n",
       "         'thought': 94,\n",
       "         'retail': 94,\n",
       "         'imports': 94,\n",
       "         'machine': 94,\n",
       "         'exports': 94,\n",
       "         'brokers': 94,\n",
       "         'longer': 93,\n",
       "         'holdings': 93,\n",
       "         'thing': 93,\n",
       "         'auction': 93,\n",
       "         'yields': 93,\n",
       "         'changed': 93,\n",
       "         'kind': 93,\n",
       "         'manufacturers': 93,\n",
       "         'majority': 93,\n",
       "         'thursday': 92,\n",
       "         'history': 92,\n",
       "         'poor': 92,\n",
       "         'done': 92,\n",
       "         'seven': 92,\n",
       "         'simply': 92,\n",
       "         'continuing': 92,\n",
       "         'subject': 92,\n",
       "         'motor': 92,\n",
       "         'stock-index': 92,\n",
       "         'fourth': 92,\n",
       "         'situation': 91,\n",
       "         'december': 91,\n",
       "         'protection': 91,\n",
       "         'turned': 91,\n",
       "         'worth': 91,\n",
       "         'war': 91,\n",
       "         'largely': 91,\n",
       "         'shareholder': 90,\n",
       "         'benefits': 90,\n",
       "         'children': 90,\n",
       "         'spokeswoman': 90,\n",
       "         'boost': 90,\n",
       "         'advanced': 90,\n",
       "         'cbs': 90,\n",
       "         'indeed': 90,\n",
       "         'areas': 90,\n",
       "         'near': 90,\n",
       "         'block': 90,\n",
       "         'rules': 90,\n",
       "         'generally': 90,\n",
       "         'men': 90,\n",
       "         'moody': 90,\n",
       "         'a.': 90,\n",
       "         'recession': 90,\n",
       "         'filing': 89,\n",
       "         'preferred': 89,\n",
       "         'electric': 89,\n",
       "         'pilots': 89,\n",
       "         'summer': 89,\n",
       "         'red': 89,\n",
       "         'settlement': 89,\n",
       "         'smaller': 89,\n",
       "         'eight': 89,\n",
       "         'planned': 89,\n",
       "         'returns': 89,\n",
       "         'arbitrage': 88,\n",
       "         'hurt': 88,\n",
       "         'housing': 88,\n",
       "         'dr.': 88,\n",
       "         'space': 88,\n",
       "         'became': 88,\n",
       "         'shearson': 88,\n",
       "         'seek': 88,\n",
       "         'form': 88,\n",
       "         'created': 87,\n",
       "         'caused': 87,\n",
       "         'note': 87,\n",
       "         ...})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def get_word_probability_dict(train_dataset: datasets.arrow_dataset.Dataset):\n",
    "    '''\n",
    "    args: \n",
    "        train_dataset: a Dataset object that can be iterated to get all the sentences\n",
    "    output:\n",
    "        word_prob_dict: a dictionary containing the word probabilities (and outputing zero for non-seen tokens)\n",
    "    '''\n",
    "    word_prob_dict = Counter()\n",
    "    for sentence in train_dataset['sentence']:\n",
    "        word_prob_dict.update(sentence.split())\n",
    "\n",
    "    return word_prob_dict\n",
    "\n",
    "word_prob_dict = get_word_probability_dict(ptb_train)\n",
    "word_prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d403e",
   "metadata": {},
   "source": [
    "Let's also get a sense of how high the top-k probabilities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37d958f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 40612),\n",
       " ('<unk>', 35814),\n",
       " ('<stop>', 33306),\n",
       " ('N', 25940),\n",
       " ('of', 19459),\n",
       " ('to', 18896),\n",
       " ('a', 16899),\n",
       " ('in', 14472),\n",
       " ('and', 14013),\n",
       " (\"'s\", 7850),\n",
       " ('for', 7108),\n",
       " ('that', 7100),\n",
       " ('$', 5954),\n",
       " ('is', 5923),\n",
       " ('it', 4866),\n",
       " ('said', 4840),\n",
       " ('on', 4493),\n",
       " ('at', 3946),\n",
       " ('by', 3939),\n",
       " ('as', 3860)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_prob_dict.items(), key=lambda item: item[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4018d93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.Counter"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_prob_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b408c2",
   "metadata": {},
   "source": [
    "#### Now let's analyze the Unigram language model for different sequences. We first create a function that can output the probability for a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc167228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_lm_seq_probability(input_sentence: str,\n",
    "                               word_prob_dict: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: The input sequence string. Here we assume\n",
    "        word_prob_dict: A dictionary containing the probability for a given token\n",
    "    output:\n",
    "        probability: The probability of the input_sentence according to the Unigram language model\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    num_words = sum(word_prob_dict.values())\n",
    "    probabilities = [word_prob_dict[word] / num_words for word in input_sentence.split()]\n",
    "    # print(probabilities)\n",
    "    probability = np.prod(probabilities)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427e046",
   "metadata": {},
   "source": [
    "#### Let's investigate a major issue with Unigram language model. What are the probabilities for the two following sequences?\n",
    "- the the the the \\<stop>\n",
    "- i love computer science \\<stop>\n",
    "\n",
    "DIscussion: How can we avoid having large probability values for sequences like `the the the <stop>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c753d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability for seq1 is 4.0260352626148156e-07, and for seq2 is 2.3386075741772593e-17\n"
     ]
    }
   ],
   "source": [
    "seq1 = \"the the the the <stop>\"\n",
    "seq2 = \"i love computer science <stop>\"\n",
    "\n",
    "prob_seq1 = unigram_lm_seq_probability(seq1, word_prob_dict)\n",
    "prob_seq2 = unigram_lm_seq_probability(seq2, word_prob_dict)\n",
    "print(f\"probability for seq1 is {prob_seq1}, and for seq2 is {prob_seq2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06747969",
   "metadata": {},
   "source": [
    "#### Now let's formally evaluate the Unigram model in terms of perplexity. We first compute the entropy as the average negative log-likelihood:\n",
    "$$H(W_{test}âˆ£M)= \\frac{1}{|W_{test}|} \\sum_{w\\in W_{test}} âˆ’log_2P(wâˆ£M)$$\n",
    ", where $W_{test}$ is the input sequence and M is the Unigram language model. (note that the logarithm is in base 2).\n",
    "\n",
    "In order to get a reliable value, we will do the above calculation for all the sentences in `ptb_test` dataset and then an average is taken over all these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa48300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_lm_entropy(input_sentence: str,\n",
    "                           word_prob_dict: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: the input string that we would like to have its respective entropy value.\n",
    "        word_prob_dict: A dictionary containing the probability for a given token\n",
    "    output:\n",
    "        entropy: entropy value as defined above\n",
    "    '''\n",
    "    num_words = sum(word_prob_dict.values())\n",
    "\n",
    "    entropy = 1/len(input_sentence.split()) * np.sum([-np.log2(word_prob_dict[word] / num_words) if word_prob_dict[word] != 0 else 0 for word in input_sentence.split()])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07afa63",
   "metadata": {},
   "source": [
    "Now compute the average entropy for all the sentences in the `ptb_test` given above function, and then compute the average entropy. Then compute the perplexity as $2^{\\bar{H}}$, where $\\bar{H}$ is the average perplexity over the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac66be2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity for the Unigram language model is 679.6126244532783\n"
     ]
    }
   ],
   "source": [
    "def get_unigram_lm_perplexity(test_dataset: datasets.arrow_dataset.Dataset,\n",
    "                              word_prob_dict: dict):\n",
    "    '''\n",
    "    args:\n",
    "        test_dataset: the test dataset samples are used to compute the perplexity for the Unigram LM.\n",
    "        word_prob_dict: A dictionary containing the probability for a given token\n",
    "    output:\n",
    "        perplexity: entropy value as defined above\n",
    "    '''  \n",
    "    avg_entropy = np.sum([get_unigram_lm_entropy(sentence, word_prob_dict) for sentence in test_dataset['sentence']]) / len(test_dataset['sentence'])\n",
    "    perplexity = 2**avg_entropy\n",
    "\n",
    "    return perplexity\n",
    "      \n",
    "unigram_lm_perplexity = get_unigram_lm_perplexity(ptb_test, word_prob_dict)\n",
    "print(f\"The perplexity for the Unigram language model is {unigram_lm_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3cec0a",
   "metadata": {},
   "source": [
    "As discussed in the lectures, the models with lower perplexities are desired; however, we should be careful when comparing language models with different vocabualry sizes.\n",
    "#### In the `ptb_train` dataset, replace every token that is appearing less than 10 times with the `<unk>` token. (Note: the same token replacement should be done for the test dataset). What is the Unigram language model perplexity for the new dataset?\n",
    "Discussion: What would happen to the vocabulary size and perplexity as we increase the rare token threshold to higher values? (instead of 10 here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a14c853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33306/33306 [00:59<00:00, 556.36 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8343/8343 [00:15<00:00, 546.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_rare_token(train_dataset: datasets.arrow_dataset.Dataset,\n",
    "                      test_dataset: datasets.arrow_dataset.Dataset,\n",
    "                      rare_token_threshold: int):\n",
    "    '''\n",
    "    Note that the tokens that are considered rare here, are identified based on the train_dataset, so that\n",
    "    we have the same token mapping (to <unk>) for both the train and test datasets. \n",
    "    args:\n",
    "        train_dataset: the input dataset where its rare tokens has to be replaced with <unk> token.\n",
    "        rare_token_threshold: every word that is appearing less than this threshold in the train dataset will\n",
    "                              be replace with the <unk> token\n",
    "    output:\n",
    "        cleaned_train_dataset: the cleaned train dataset where rare tokens are replace with <unk> token.\n",
    "        cleaned_test_dataset: the cleaned test dataset where rare tokens are replace with <unk> token.\n",
    "    '''\n",
    "    \n",
    "    rare_tokens = [word for word in word_prob_dict if word_prob_dict[word] < rare_token_threshold]\n",
    "    cleaned_train_dataset = train_dataset.map(lambda x: {'sentence': ' '.join(['<unk>' if word in rare_tokens else word for word in x['sentence'].split()])})\n",
    "    cleaned_test_dataset = test_dataset.map(lambda x: {'sentence': ' '.join(['<unk>' if word in rare_tokens else word for word in x['sentence'].split()])})\n",
    "    \n",
    "    \n",
    "    return cleaned_train_dataset, cleaned_test_dataset\n",
    "\n",
    "cleaned_train_dataset, cleaned_test_dataset = remove_rare_token(train_dataset=ptb_train,\n",
    "                                                                test_dataset=ptb_test,\n",
    "                                                                rare_token_threshold=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09a7d3",
   "metadata": {},
   "source": [
    "##### Now, follow similar steps to compute the perplexity given the two new datasets (`cleaned_train_dataset` and `cleaned_test_dataset`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2083f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity for the Unigram language model after replacing rare tokens is  461.20957602855543\n"
     ]
    }
   ],
   "source": [
    "cleaned_unigram_lm_perplexity = -1\n",
    "\n",
    "cleaned_word_prob_dict = get_word_probability_dict(cleaned_train_dataset)\n",
    "cleaned_unigram_lm_perplexity = get_unigram_lm_perplexity(cleaned_test_dataset, cleaned_word_prob_dict)\n",
    "\n",
    "print(\"The perplexity for the Unigram language model after replacing rare tokens is \",\n",
    "      cleaned_unigram_lm_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe9eac",
   "metadata": {},
   "source": [
    "## 1.2 Bi-gram Language Model <a name='bigram_lm'></a>\n",
    "In the Bi-gram language model, we pick/generate next token conditioned only on the previous token. Therefore, for an arbitrary sequence $x_1x_2~...x_n$, its respective probability becomes:\n",
    "$$p(x_1x_2~...x_n) = p(x_1) ~\\Pi_{i=2} ^n p(x_i|x_{i-1})$$\n",
    "Let's use the same dataset (`Penn Treebank`) to evaluate this model's perplexity. (We use the dataset that already has the `<stop>` token at the end).\n",
    "\n",
    "We estimate $p(x_i|x_{i-1})$ as the $\\frac{count(x_{i-1},~x_i)}{count(x_{i-1})}$ according to the training dataset frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96966fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_order_conditional_probabilities(train_dataset: datasets.arrow_dataset.Dataset):\n",
    "    '''\n",
    "    In this function the conditional probabilities have to be computed based train_dataset. The output of the\n",
    "    function is a dictionary having keys like (x_{i-1}, x_i) as a tuple and the value being p(x_i|x_{i-1}).\n",
    "    args:\n",
    "        train_dataset: a Dataset object that can be iterated to get all the sentences\n",
    "    output:\n",
    "        word_prob_dict: \n",
    "        first_order_condition_prob: a dictionary having containing the first order conditional probabilities\n",
    "                                    as discussed above.\n",
    "        word_prob_dict: a dictionary containing the word probabilities\n",
    "    '''\n",
    "    first_order_condition_prob = defaultdict(float) # in order to get zeroes \n",
    "    # let's first get the word frequencies (later used for computation of conditional probabilities)\n",
    "    word_prob_dict = get_word_probability_dict(train_dataset)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return word_prob_dict, first_order_condition_prob\n",
    "\n",
    "word_prob_dict, first_order_condition_prob = get_first_order_conditional_probabilities(ptb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89cb406",
   "metadata": {},
   "source": [
    "#### Now let's analyze the Bi-gram language model for different sequences. We first create a function that can output the probability for a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e459cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_lm_seq_probability(input_sentence: str,\n",
    "                              word_prob_dict: dict,\n",
    "                              first_order_condition_prob: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: The input sequence string. Here we assume\n",
    "        word_prob_dict: a dictionary containing the word probabilities\n",
    "        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n",
    "                                    as discussed in the previous function.\n",
    "    output:\n",
    "        probability: The probability of the input_sentence according to the Bi-gram language model\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87509ef",
   "metadata": {},
   "source": [
    "Let's investigate a major issue with higher order language models.\n",
    "#### Compute the probabilities for all the sequences in `ptb_test` dataset, and compute the minimum value among these probablities. What would be the perplexity for the dataset given these values?\n",
    "Discussion: How can we avoid this **overfitting** to train dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_test_probabilities = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"{bigram_test_probabilities.count(0)/len(ptb_test)*100}% of samples in the test set have zero probability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced914f",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "As we saw above, due to having new pair of consecutive words in the test dataset, we might have zero probabilities for some sequences. Therefore, as discussed in the lectures, in order to have a meaningful perplexity for N-gram language models, we need to smooth the probabilities to have non-zero values for non-seen sequences. In this exercise, we use Laplace smoothing as defined below:\n",
    "$$P(x_i|x_{i-1}) = \\frac{count(x_{i-1},~x_i) + \\alpha}{count(x_{i-1}) + \\alpha ~|V|}$$\n",
    ", where $\\alpha$ is the smoothing parameter, and $|V|$ is the (train dataset) vocabulary size.\n",
    "\n",
    "#### Let's recompute the conditional probabilities using Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d808944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smoothed_first_order_conditional_probabilities(train_dataset: datasets.arrow_dataset.Dataset,\n",
    "                                                       smoothing_alpha: float):\n",
    "    '''\n",
    "    In this function the conditional probabilities have to be computed based on train_dataset. The output\n",
    "    of the function is a dictionary having keys like (x_{i-1}, x_i) as a tuple and the\n",
    "    value being p(x_i|x_{i-1}).\n",
    "    args:\n",
    "        train_dataset: a Dataset object that can be iterated to get all the sentences\n",
    "        smoothing_alpha: The alpha parameter used in the Laplace smoothing.\n",
    "    output:\n",
    "        word_prob_dict: a dictionary containing the word probabilities \n",
    "        first_order_condition_prob: a dictionary containing the smoothed first order\n",
    "                                    conditional probabilities as discussed above.\n",
    "    '''\n",
    "    first_order_condition_prob = defaultdict(float)  # Note that we shouldn't get zeros for unseen events.\n",
    "    # let's first get the word probabilities (later used for computation of conditional probabilities)\n",
    "    word_prob_dict = get_word_probability_dict(train_dataset)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE here\n",
    "    \n",
    "    return word_prob_dict, first_order_condition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_bigram_lm_seq_probability(input_sentence: str,\n",
    "                                       word_prob_dict: dict,\n",
    "                                       word_frequency_dict: dict,\n",
    "                                       first_order_condition_prob: dict,\n",
    "                                       smoothing_alpha: float):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: The input sequence string. Here we assume\n",
    "        word_prob_dict: a dictionary containing the word probabilities\n",
    "        word_frequency_dict: a dictionary containing the frequency for every word in vocabulary\n",
    "        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n",
    "                                    as discussed in the previous function.\n",
    "        smoothing_alpha: The alpha parameter used in the Laplace smoothing.\n",
    "    output:\n",
    "        probability: The probability of the input_sentence according to the Bi-gram language model\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd77e6c",
   "metadata": {},
   "source": [
    "#### Assuming $\\alpha=0.01$ for the smoothing, use the previous function and `bigram_lm_seq_probability` to compute the sequence probabilities for all the sentences in the `ptb_test` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c76c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_bigram_test_probabilities = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"{smoothed_bigram_test_probabilities.count(0)/len(ptb_test)*100}% of samples in the test set have zero probability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b38b8",
   "metadata": {},
   "source": [
    "If the perplexity for a given sequence is computed as below, compute the Bigram language model perplexity over `ptb_test` dataset over all the sentences ($\\alpha=0.01)$:\n",
    "$$Perplexity(x_1x_2...x_n) = p(x_1x_2...x_n)^{-1/n}$$\n",
    ", where $p(x_1x_2...x_n)$ is the probability assigned to $x_1x_2...x_n$ sequence by the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3665f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_lm_perplexity = -1\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"Bigram language model perplexity is {bigram_lm_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd753ef",
   "metadata": {},
   "source": [
    "Repeat the same steps but for `cleaned_train_dataset` and `cleaned_test_dataset` datasets where rare tokens (with frequency less than 10) are replaced with `<unk>` token. Do we have a better or a worse perplexity compared to the previous computed perplexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b98f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"(cleaned) Bigram language model perplexity is {cleaned_bigram_lm_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea844a",
   "metadata": {},
   "source": [
    "## 1.3 Tri-gram Language Model <a name='trigram_lm'></a>\n",
    "In the Tri-gram language model, we pick/generate next token conditioned only on the two previous tokens. Therefore, for an arbitrary sequence $x_1x_2~...x_n$, its respective probability becomes:\n",
    "$$p(x_1x_2~...x_n) = p(x_1) p(x_2|x_1) ~\\Pi_{i=3} ^n p(x_i|x_{i-2}x_{i-1})$$\n",
    "Let's use the same dataset (`Penn Treebank`) to evaluate this model's perplexity. (We use the dataset that already has the `<stop>` token at the end of each sentence).\n",
    "\n",
    "\n",
    "We estimate $p(x_i|x_{i-1}x_{i-2})$ using the Laplace smoothing with $\\alpha=3 \\cdot 10^{-3}$. First let's write a function that computes these conditional probabilities for the Tri-gram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smoothed_second_order_conditional_probabilities(train_dataset: datasets.arrow_dataset.Dataset,\n",
    "                                                        smoothing_alpha: float):\n",
    "    '''\n",
    "    In this function the conditional probabilities have to be computed based on train_dataset. The output\n",
    "    of the function is a dictionary having keys like (x_{i-2}, x_{i-1}, x_i) as a tuple and the\n",
    "    value being p(x_i | x_{i-2} x_{i-1}).\n",
    "    args:\n",
    "        train_dataset: a Dataset object that can be iterated to get all the sentences\n",
    "        smoothing_alpha: The alpha parameter used in the Laplace smoothing.\n",
    "    output:\n",
    "        word_prob_dict: a dictionary containing the word probabilities \n",
    "        first_order_condition_prob: a dictionary containing the smoothed first order\n",
    "                                    conditional probabilities.\n",
    "        second_order_condition_prob: a dictionary containing the smoothed second order\n",
    "                                     conditional probabilities.\n",
    "    '''\n",
    "    smoothed_second_order_condition_prob = defaultdict(float)  # Note that we shouldn't get zeros for unseen probabilies.\n",
    "    \n",
    "    # let's first get the 0th and 1st order conditional probabilities\n",
    "    (word_prob_dict, first_order_condition_prob) = get_smoothed_first_order_conditional_probabilities(\n",
    "        train_dataset, smoothing_alpha)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return word_prob_dict, first_order_condition_prob, second_order_condition_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a664f",
   "metadata": {},
   "source": [
    "#### Now let's analyze the Tri-gram language model for different sequences. We first create a function that can output the probability for a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_trigram_lm_seq_probability(input_sentence: str,\n",
    "                                        word_prob_dict: dict,\n",
    "                                        word_frequency_dict: dict,\n",
    "                                        bigram_frequency_dict: dict,\n",
    "                                        first_order_condition_prob: dict,\n",
    "                                        second_order_condition_prob: dict,\n",
    "                                        smoothing_alpha: float):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: The input sequence string. Here we assume\n",
    "        word_prob_dict: a dictionary containing the word probabilities\n",
    "        word_frequency_dict: a dictionary containing the frequency for every word in vocabulary\n",
    "        bigram_frequency_dict: a dictionary containing the frequency for every bigram in vocabulary\n",
    "        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n",
    "                                    as discussed earlier.\n",
    "        second_order_condition_prob: a dictionary containing the second order conditional probabilities\n",
    "                                     as discussed in the previous function.\n",
    "    output:\n",
    "        probability: The probability of the input_sentence according to the Bi-gram language model\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE    \n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16865742",
   "metadata": {},
   "source": [
    "#### Now let's compute the probability for sequences in the test dataset, assuming $\\alpha=3\\cdot10^{-3}$ has been used in the Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36feefd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_trigram_test_probabilities = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"{smoothed_trigram_test_probabilities.count(0)/len(ptb_test)*100}% of samples in the test set have zero probability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ddd040",
   "metadata": {},
   "source": [
    "Now we compute the perplexity on the `ptb_test` dataset for the tri-gram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed3d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trigram_lm_perplexity = -1\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"Trigram language model perplexity is {Trigram_lm_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c59c8",
   "metadata": {},
   "source": [
    "Repeat the same steps but for `cleaned_train_dataset` and `cleaned_test_dataset` datasets where rare tokens (with frequency less than 10) are replaced with `<unk>` token. Do we have a better or a worse perplexity compared to the previous computed perplexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b456be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e69cc",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    " - How are the three discussed models performance compare to each other?\n",
    " - What is the cost of using N-gram language models for even larger N values?\n",
    " - What is the effect of vocabulary size on models' perplexities? Can we compare models with different vocabulary sizes?\n",
    " - What is the perplexity of a language model (vocabulary size of |V|) that given any context (i.e., $x_1 x_2 ... x_{n-1}$) assigns uniform probabilities (for all the tokens in the vocabulary) for the next token? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641578b1",
   "metadata": {},
   "source": [
    "## 2. Task B: Neural Language Models <a name='neural_lm'></a>\n",
    "\n",
    "In this exercise, we will better understand the functioning of some simple neural language models. We first start with a fixed-window neural language model. In the following subsection, we will investigate an RNN-based language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cee765",
   "metadata": {},
   "source": [
    "### 2.1 Fixed-Window Neural Language Model <a name='fixed_window_lm'></a>\n",
    "This language model take as input a constant number of tokens, and then outputs a probability distribution for the next token. In this section, we assume the underlying model is a Multi-layer Perceptron (MLP) with a single hidden layer. This model doesn't have the sparsity issue of N-gram language models, but is always limited to a fixed window of tokens.\n",
    "\n",
    "In this section, we don't include the training of the model but rather we use a pretrained model on the same training dataset. We evaluate the language model over the `ptb_test` dataset, to show the power of neural language models, when compared to N-gram language models.\n",
    "\n",
    "More importantly, we use PyTorch modules in this section, so that you get more familiar with its capabilities. Throughout this exercise, we use a `window_size=3` for this model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6808da1e",
   "metadata": {},
   "source": [
    "Let's first create a dataset of all consecutive tokens of length `window_size` from the `ptb_train` dataset. you can read more about PyTorch datasets and how to create a custom dataset  [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "window_size = 3\n",
    "vocabulary_size = 10000\n",
    "word_emb_dim = 100\n",
    "hidden_dim = 100\n",
    "\n",
    "\n",
    "class FixedWindowDataset(Dataset):\n",
    "    # read more about custom datasets at https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "    def __init__(self,\n",
    "                 train_dataset: datasets.arrow_dataset.Dataset,\n",
    "                 test_dataset: datasets.arrow_dataset.Dataset,\n",
    "                 window_size: int,\n",
    "                 vocabulary_size: int\n",
    "                ):\n",
    "        self.prepared_train_dataset = self.prepare_fixed_window_lm_dataset(train_dataset, window_size + 1)\n",
    "        self.prepared_test_dataset = self.prepare_fixed_window_lm_dataset(test_dataset, window_size + 1)\n",
    "        \n",
    "        dataset_vocab = self.get_dataset_vocabulary(train_dataset)\n",
    "        # defining a dictionary that simply maps tokens to their respective index in the embedding matrix\n",
    "        self.word_to_index = {word: idx for idx,word in enumerate(dataset_vocab)}\n",
    "        self.index_to_word = {idx: word for idx,word in enumerate(dataset_vocab)}\n",
    "        \n",
    "        assert vocabulary_size >= len(dataset_vocab) , f\"The dataset vocab size is {len(dataset_vocab)}!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prepared_train_dataset)\n",
    "    \n",
    "    def get_encoded_test_samples(self):\n",
    "        all_token_lists = [sample.split() for sample in self.prepared_test_dataset]\n",
    "        all_token_ids = [[self.word_to_index.get(word, self.word_to_index[\"<unk>\"])\n",
    "                          for word in token_list[:-1]]\n",
    "                         for token_list in all_token_lists\n",
    "                        ]\n",
    "        all_next_token_ids = [self.word_to_index.get(token_list[-1], self.word_to_index[\"<unk>\"]) for \n",
    "                              token_list in all_token_lists]\n",
    "        return torch.tensor(all_token_ids), torch.tensor(all_next_token_ids)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # here we need to transform the data to the format we expect at the model input\n",
    "        token_list = self.prepared_train_dataset[idx].split()\n",
    "        # having a fallback to <unk> token if an unseen word is encoded.\n",
    "        token_ids = [self.word_to_index.get(word, self.word_to_index[\"<unk>\"]) for word in token_list[:-1]]\n",
    "        next_token_id = self.word_to_index.get(token_list[-1], self.word_to_index[\"<unk>\"])\n",
    "        return torch.tensor(token_ids), torch.tensor(next_token_id)\n",
    "    \n",
    "    def decode_idx_to_word(self, token_id):\n",
    "        return [self.index_to_word[id_.item()] for id_ in token_id]\n",
    "    \n",
    "    def get_dataset_vocabulary(self, train_dataset: datasets.arrow_dataset.Dataset):\n",
    "        vocab = sorted(set(\" \".join([sample[\"sentence\"] for sample in train_dataset]).split()))\n",
    "        # we also add a <start> token to include initial tokens in the sentences in the dataset\n",
    "        vocab += [\"<start>\"]\n",
    "        return vocab\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_fixed_window_lm_dataset(target_dataset: datasets.arrow_dataset.Dataset,\n",
    "                                        window_size: int):\n",
    "        '''\n",
    "        Please note that for the very first tokens, they will be added like \"<start> <start> Token#1\".\n",
    "        args:\n",
    "            target_dataset: the target dataset where its consecutive tokens of length 'window_size' should be extracted\n",
    "            window_size: the window size for the language model\n",
    "        output:\n",
    "            prepared_dataset: a list of strings each containing 'window_size' tokens.\n",
    "        '''\n",
    "        \n",
    "        prepared_dataset = []\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return prepared_dataset\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_window_dataset = FixedWindowDataset(ptb_train, ptb_test, window_size, vocabulary_size)\n",
    "\n",
    "# let's create a simple dataloader for this dataset\n",
    "train_dataloader =  DataLoader(fixed_window_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44e90d",
   "metadata": {},
   "source": [
    "Now, let's define the underlying PyTorch model for the language model. You can read more about PyTorch models [here](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html).\n",
    "\n",
    "**Note**: Here in the forward pass, we compute the negative log-likelihood after passing through the MLP layers. Here we use `torch.nn.LogSoftmax`, as it's numerically more stable than doing seperately `softmax` followed by taking its logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class Fixed_window_language_model(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, window_size, vocab_size=10000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, emb_dim) # word embeddings\n",
    "        self.linear1 = torch.nn.Linear(window_size * emb_dim, hidden_dim) # first linear layer\n",
    "        self.activation_func = torch.tanh # the activation function\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, vocab_size) # second linear layer\n",
    "        \n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.criterion = torch.nn.NLLLoss()\n",
    "     \n",
    "    def forward(self, input_ids, labels):\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        concat_input_embed = inputs_embeds.reshape(-1, self.emb_dim * self.window_size)\n",
    "        hidden_state = self.activation_func( self.linear1(concat_input_embed) )\n",
    "        logits = self.log_softmax( self.linear2(hidden_state) )\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae5397",
   "metadata": {},
   "source": [
    "Now let's see how easy it is to train a model with PyTorch! (we provide a trained model in the cell after train, so that you can just start using the model without going through the time-consuming training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93779901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model\n",
    "model_fixed_window = Fixed_window_language_model(emb_dim=word_emb_dim, hidden_dim=hidden_dim,\n",
    "                                                 window_size=window_size, vocab_size=vocabulary_size)\n",
    "\n",
    "# defining the optimizer\n",
    "optimizer = optim.SGD(model_fixed_window.parameters(),\n",
    "                      lr=0.005,\n",
    "                      momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77626d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # get the inputs; data is a tuple of (context, target)\n",
    "        context, target = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        loss = model_fixed_window(context, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 5000 == 4999. :    # print every 5000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 5000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# saving the trained model\n",
    "torch.save(model_fixed_window.state_dict(), \"fixed_window_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0483dd01",
   "metadata": {},
   "source": [
    "We provide a trained model, so that you can start using it right away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba51b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_window_checkpoint_file = \"fixed_window_model.pt\"\n",
    "model_fixed_window.load_state_dict(torch.load(fixed_window_checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b806f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context and 'target' ids (target is the next word after the context)\n",
    "test_token_ids, test_target_ids = fixed_window_dataset.get_encoded_test_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87e283d",
   "metadata": {},
   "source": [
    "We now have the `test_token_ids`, `test_target_ids` tensors for the test dataset. The `test_token_ids` are the context ids and `test_target_ids` are the respective **next token** (a.k.a. target here) for these contexts.\n",
    "#### Using the trained model, implement a function that can output the loss for the discussed test dataset. How can we generally decide if the model is overfitted to the train dataset or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_dataset_loss(model: torch.nn.Module,\n",
    "                               test_token_ids: torch.Tensor,\n",
    "                               test_target_ids: torch.Tensor):\n",
    "    '''\n",
    "    args:\n",
    "        model: fixed-window language model\n",
    "        test_token_ids: the context ids in a single tensor.\n",
    "        test_target_ids: the target ids (next token after the context) in a single tensor.\n",
    "    output:\n",
    "        avg_test_loss: The average loss of model over test dataset.\n",
    "    '''\n",
    "    batch_size = 4\n",
    "    test_loss = []\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return avg_test_loss\n",
    "\n",
    "\n",
    "test_dataset_loss = generate_test_dataset_loss(model_fixed_window, test_token_ids, test_target_ids)\n",
    "print(f\"Test dataset loss is {test_dataset_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d7476",
   "metadata": {},
   "source": [
    "#### Using the trained fixed-window model, implemention a function that can output entropy for a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seqeuence_entropy_fixed_window_lm(model: torch.nn.Module,\n",
    "                                              input_sequence: str,\n",
    "                                              window_size: int,\n",
    "                                              word_to_idx: dict):\n",
    "    '''\n",
    "    Note that e.g., in order to get the first token probability, you need to pass a sequence\n",
    "    like \"<start> <start> <start>\" (prefix padding) to the neural model. In a similar fashion, we need to pass\n",
    "    \"<start> <start> TOKEN#1\" for getting the probability of the second token.\n",
    "    args:\n",
    "        model: fixed-window language model\n",
    "        input_sequence: the sequence for which we want to calculate the probability\n",
    "        window_size: the size of window for the language model\n",
    "        word_to_idx: a mapping from words to the embedding indices (to encode tokens before being\n",
    "                     passed to model). You can get this dict from 'fixed_window_dataset.word_to_index'\n",
    "    output:\n",
    "        sequence_entropy: the entropy for the input sequence using the trained model\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return sequence_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d055835",
   "metadata": {},
   "source": [
    "#### Compute the perplexity for the trained fixed-window language model over `ptb_test` dataset using the previous function. How does it perform compared to N-gram language models we discussed earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4289886",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = -1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"The fixed-window model perplexity over test dataset is {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c3d89",
   "metadata": {},
   "source": [
    "### 2.2 RNN-based Language Model <a name='rnn_lm'></a>\n",
    "To address the need for a neural architecture that can proceed with any length input (as opposed to the fixed-window model that can only process a fixed number of tokens), we implement the Recurrent Neural Network (RNN). The core idea behind is that we can apply the same weight W repeatedly.\n",
    "\n",
    "An advatange of RNN model compared to fixed-window langauage model is that we can pass a given sentence at once, instead of passing it in many windows of size `window_size`. Moreover, the language model has the ability to look behind further that a fixed number of tokens.\n",
    "\n",
    " As we already did a neural model training exercise for the previous neural model, we only provide a trained LM at this section, so that you can focus only on the analysis part.\n",
    " \n",
    "You can find the dataset structure as well as the RNN architecture in the `rnn_utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc631bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn_utils import RNNDataset, RNN_language_model\n",
    "\n",
    "vocabulary_size = 10000\n",
    "word_emb_dim = 200\n",
    "hidden_dim = 200\n",
    "\n",
    "rnn_dataset = RNNDataset(ptb_train, ptb_test, vocabulary_size)\n",
    "\n",
    "# if gpu is available, we puts the model on it \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Here we need a <pad> token for the RNN model, in order to have a batch of sequences with difference sizes \n",
    "pad_idx = rnn_dataset.pad_idx # the index for <pad> token\n",
    "rnn_model = RNN_language_model(vocab_size=vocabulary_size, emb_dim=word_emb_dim, hidden_dim=hidden_dim,\n",
    "                               pad_idx=pad_idx)\n",
    "rnn_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25137a9f",
   "metadata": {},
   "source": [
    "load the model weights using the state_dict in `rnn_model.pt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52adb1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06eb967",
   "metadata": {},
   "source": [
    "As the training of an RNN model is time-consuming, we provide a trained language model on this dataset (`rnn_model.pt`), so that you can just analyze the model performance here.\n",
    "As mentioned above, as RNN can get sequences with varying lengths, the input sequences should be padded with a special token like `<pad>`, so that we can create a batch of sentences. The output of the defined RNN model (see the architecture detail `rnn_utils.py`) is the model's entropy over the input data.\n",
    "\n",
    "#### First get the encoded test samples of `ptb_test` dataset, and then pass these (already padded) sentences to the RNN model to get the respective entropy values. Compute the perplexity of the model and compare it with previous approaches.\n",
    "**HINT**: You can use the `get_encoded_test_samples` function of `rnn_dataset` to get encoded test samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perplexity = -1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"The model perplexity is {test_perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8ced340a52f9326f5856e1d63a73f97bd9f0a225610b549ff7b502d766a19ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
