{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qU4Y9G1pZFW"
      },
      "source": [
        "# ðŸ“š  Exercise Session - Week 5\n",
        "\n",
        "Welcome to Week 5 exercise session of CS552-Modern NLP!\n",
        "\n",
        "We will continue playing with `DistilBert` this week, and learn about the dataset biases and prompting.\n",
        "\n",
        "[Part 1: Biases](#bias0)\n",
        "- [1.1 Hypothesis only NLI](#bias1)\n",
        "\n",
        "[Part 2: Prompting](#prompt0)\n",
        "- [2.1 Zero-shot Prompting](#promp1)\n",
        "- [2.2 Few-shot Prompting](#promt2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97PgcwMKpZY4"
      },
      "source": [
        "<a name=\"bias0\"></a>\n",
        "## 1. Biases\n",
        "\n",
        "Recall our knowledge about the NLI tasks, the model would be given a pair of sentence: `(premise, hypothesis)`, and needs to judge the relationship between them. Specifically, given the *premise*, if the *hypothesis* is **true (entailment)**, **false (condradiction)**, or **neither (neutral)**. Idealy, The label of the hypothesis should be entirely based upon the given premise. However, *if the model is able to correctly guess the label without seeing the premise, it is likely detecting biased statisitcal patterns that are undesirable*, such as tendency to use certain words among different classes (ex: using negation words such as 'not' for the contradiction label).\n",
        "\n",
        "Inspired by the paper [Hypothesis Only Baselines in Natural Language Inference](https://aclanthology.org/S18-2023.pdf), the first part of this lab will investigate a classifier's internal bias when performing the NLI task by testing its hypothesis-only performance.\n",
        "\n",
        "**`Note`** In this dataset the labels are as follows: `0-Entailment`, `1- Neutral`, and `2- Contradict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lxexYkR3pZsc"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install jsonpickle\n",
        "# !pip install datasets\n",
        "# !pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yb7ZmtDHyKAb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import jsonpickle\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import trange, tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import RandomSampler, DataLoader, SequentialSampler\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import RobertaForMaskedLM,RobertaTokenizer, RobertaForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3I7D8ul5ikM"
      },
      "source": [
        "<a name=\"bias1\"></a>\n",
        "## 1.1 Train: Hypothesis only NLI\n",
        "\n",
        "Let's firstly train a `distilbert` model on the SNLI dataset, but only access the hypothesis. We reuse the functions from the Exercise4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mgAGjzpYn-Ws"
      },
      "outputs": [],
      "source": [
        "def load_pretrained(model_name, num_labels=2):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    model = model.to('cuda:0')\n",
        "    print('Model loaded to GPU')\n",
        "  return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LnsR5KXGntJd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#Training samples:  549367\n",
            "#Test samples:  9824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded to GPU\n"
          ]
        }
      ],
      "source": [
        "train_dataset = load_dataset(\"snli\", split='train')\n",
        "test_dataset = load_dataset(\"snli\", split='test')\n",
        "train_dataset = train_dataset.filter(lambda example: example[\"label\"]!=-1)\n",
        "test_dataset = test_dataset.filter(lambda example: example[\"label\"]!=-1)\n",
        "print('#Training samples: ', len(train_dataset))\n",
        "print('#Test samples: ', len(test_dataset))\n",
        "\n",
        "tokenizer, model = load_pretrained('distilbert-base-uncased', num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sbNnn1CMnzhC"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X7pBz6KopBNN"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_nli(model, tokenizer, test_loader):\n",
        "  all_labels = None\n",
        "  all_preds = None\n",
        "\n",
        "  for b in tqdm(test_loader):\n",
        "    premise = b['premise']\n",
        "    hypothesis = b['hypothesis']\n",
        "    label = b['label']\n",
        "\n",
        "    # TODO: tokenize the text\n",
        "    inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    if torch.cuda.is_available():\n",
        "      inputs = inputs.to('cuda:0')\n",
        "      # label = label.to('cuda:0')\n",
        "\n",
        "    # TODO: run the model to make the prediction\n",
        "    with torch.no_grad():\n",
        "        pred = model(**inputs).logits.argmax(dim=-1)\n",
        "\n",
        "    if all_labels is None:\n",
        "      all_labels = label.cpu()\n",
        "      all_preds = pred.cpu()\n",
        "    else:\n",
        "      all_labels = torch.concat([all_labels, label])\n",
        "      all_preds = torch.concat([all_preds, pred.cpu()])\n",
        "\n",
        "  assert len(all_preds)==len(all_labels), 'Test Failed. Check your code!'\n",
        "  # compute f1 score between model predictions and ground-truth labels (you can use sklearn.metrics)\n",
        "  f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "  # compute accuracy score between model predictions and ground-truth labels (you can use sklearn.metrics)\n",
        "  acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "  # compute the accuracy on Entailment(label==0) samples\n",
        "  entailment_acc = accuracy_score(all_labels[all_labels==0], all_preds[all_labels==0])\n",
        "\n",
        "  # compute the accuracy on Neutral(label==1) samples\n",
        "  neutral_acc = accuracy_score(all_labels[all_labels==1], all_preds[all_labels==1])\n",
        "\n",
        "  # compute the accuracy on Contradict(label==1) samples\n",
        "  contradict_acc = accuracy_score(all_labels[all_labels==2], all_preds[all_labels==2])\n",
        "\n",
        "  print('Accuracy: ', acc*100, '%')\n",
        "  print(' -- Entailment Accuracy: ', entailment_acc*100, '%')\n",
        "  print(' -- Neutral Accuracy: ', neutral_acc*100, '%')\n",
        "  print(' -- Contradict Accuracy: ', contradict_acc*100, '%')\n",
        "  print('F1 score: ', f1)\n",
        "\n",
        "  return all_preds, all_labels, acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xbMBYWF0IHPQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [00:03<00:00, 178.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  32.69543973941368 %\n",
            " -- Entailment Accuracy:  0.0 %\n",
            " -- Neutral Accuracy:  99.13016464740603 %\n",
            " -- Contradict Accuracy:  0.6487488415199258 %\n",
            "F1 score:  0.16828325648335\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ETS: <1min on colab T4 gpu\n",
        "all_preds, all_labels, acc, f1 = evaluate_model_nli(model, tokenizer, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ezY5R5mCMs"
      },
      "source": [
        "`TODO-1`: Implement the `tokenize_function` to tokenize only the `hypothesis` in each input `examples`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XOygmXHFIOCF"
      },
      "outputs": [],
      "source": [
        "# TODO: Define a function to tokenize the text\n",
        "def tokenize_function(examples, hyp_only=True, max_length=512, device='cuda:0'):\n",
        "  '''\n",
        "  INPUT:\n",
        "    examples: input samples in the dataset\n",
        "    hyp_only: if True, only tokenize the \"hypothesis\"; tokenize both \"premise\" and \"hypothesis\" if False\n",
        "    max_length: maximal number of tokens\n",
        "    device: cuda or cpu\n",
        "  OUTPUT:\n",
        "    tokenized: tokenized sample, truncation=True, padding=True\n",
        "  '''\n",
        "  if hyp_only:\n",
        "    tokenized = tokenizer(examples['hypothesis'], truncation=True, padding=True, max_length=max_length)\n",
        "  else:\n",
        "    tokenized = tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding=True, max_length=max_length)\n",
        "  return tokenized\n",
        "\n",
        "# Tokenize the train and test data\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched = True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched = True)\n",
        "\n",
        "# Define a data collator to handle padding\n",
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vQcmZaRsJj4P"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jerem\\anaconda3\\envs\\modern_nlp\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import the trainer and training arguments\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Define the output directory and other training arguments\n",
        "output_dir_name = \"snli-hyp-distilbert\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "   output_dir = output_dir_name,\n",
        "   learning_rate = 2e-5,\n",
        "   per_device_train_batch_size = 16,\n",
        "   per_device_eval_batch_size = 16,\n",
        "   num_train_epochs = 1,\n",
        "   max_steps = 5000,\n",
        "   weight_decay = 0.01,\n",
        "   save_strategy = \"steps\",\n",
        "   save_steps = 500,\n",
        "   push_to_hub = False,\n",
        ")\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = Trainer(\n",
        "   model = model,\n",
        "   args = training_args,\n",
        "   train_dataset = tokenized_train,\n",
        "   tokenizer = tokenizer,\n",
        "   data_collator = data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "C0qUHJeC0h6q"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cca1a0381774be28c171c545dd8c2e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory snli-hyp-distilbert\\checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9445, 'grad_norm': 5.129725456237793, 'learning_rate': 1.8e-05, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory snli-hyp-distilbert\\checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8649, 'grad_norm': 5.030688285827637, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory snli-hyp-distilbert\\checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8482, 'grad_norm': 4.481287002563477, 'learning_rate': 1.4e-05, 'epoch': 0.04}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory snli-hyp-distilbert\\checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.814, 'grad_norm': 4.831885814666748, 'learning_rate': 1.2e-05, 'epoch': 0.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory snli-hyp-distilbert\\checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8096, 'grad_norm': 5.2172651290893555, 'learning_rate': 1e-05, 'epoch': 0.07}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory snli-hyp-distilbert\\checkpoint-3000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8015, 'grad_norm': 6.034611701965332, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.09}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory snli-hyp-distilbert\\checkpoint-3500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7865, 'grad_norm': 6.99624490737915, 'learning_rate': 6e-06, 'epoch': 0.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory snli-hyp-distilbert\\checkpoint-4000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7952, 'grad_norm': 7.441339015960693, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory snli-hyp-distilbert\\checkpoint-4500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7816, 'grad_norm': 5.0303850173950195, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.13}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory snli-hyp-distilbert\\checkpoint-5000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7761, 'grad_norm': 7.514829158782959, 'learning_rate': 0.0, 'epoch': 0.15}\n",
            "{'train_runtime': 133.7896, 'train_samples_per_second': 597.954, 'train_steps_per_second': 37.372, 'train_loss': 0.8222052978515625, 'epoch': 0.15}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5000, training_loss=0.8222052978515625, metrics={'train_runtime': 133.7896, 'train_samples_per_second': 597.954, 'train_steps_per_second': 37.372, 'train_loss': 0.8222052978515625, 'epoch': 0.15})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QuM-1JVa9mkm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [00:03<00:00, 182.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  57.21701954397395 %\n",
            " -- Entailment Accuracy:  57.86817102137767 %\n",
            " -- Neutral Accuracy:  67.62969866418143 %\n",
            " -- Contradict Accuracy:  46.1847389558233 %\n",
            "F1 score:  0.5694323065760162\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ETS: <1min on colab T4 gpu\n",
        "all_preds, all_labels, acc, f1 = evaluate_model_nli(model, tokenizer, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihq84X2nSe2Q"
      },
      "source": [
        "As you see, the model is able to correctly guess the labels of almost **70%** of the NLI hypotheses without seeing what the premise is.\n",
        "\n",
        "Question: What do you think are ways that biases can be mitigated? Think about both the data collection process and model training for places where one can intervene."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6NTm7cJ8dqb"
      },
      "source": [
        "<a name=\"prompt0\"></a>\n",
        "## 2. Prompting\n",
        "\n",
        "The following sections will he based off the papers [Exploiting Cloze Questions for Few Shot Text Classification and Natural\n",
        "Language Inference](https://arxiv.org/pdf/2001.07676.pdf) and [How Many Data Points is a Prompt Worth?](https://arxiv.org/pdf/2103.08493.pdf).\n",
        "\n",
        "The first paper introduced Pattern Exploiting Training (PET), in which a NLP task is reformulated to a cloze style task for few shot learning. We will go into this a little more during the few-shot section of this lab. What you need to know for now, is that instead of training a model with a classification head, these models have a LM head to perform a classification task. Unlike language modeling, instead of predicting a word from the whole vocaulary, we are predicting a word from a list of verbalizers, where a word in the vocab is mapped to each label.\n",
        "\n",
        "\n",
        "We will be looking at classification tasks (NLI and sentiment) as they need only using one mask/single word verbalizers, however this paradigm can be extended to other tasks, with multiword verbalizers.\n",
        "\n",
        "First lets try **zero-shots prompting**. We will use `Roberta-large` for this section and investigate an easier `sentiment-analysis` task on IMDB dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EtDDLw-ryYEu"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6a796a4607c47108543bb43f6b8d488",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jerem\\anaconda3\\envs\\modern_nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jerem\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c553f01d7cf94c4ea5494d3465932cc0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b98a1a74481543789f4aa0919622984d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c2b624d3cc84976ada95032c3f36a82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "490d68af413641efb2fdbaa56a7b3258",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f16eca2f2b2482ea55ac4810eff341a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_dataset = load_dataset('imdb', split='test')\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
        "model = RobertaForMaskedLM.from_pretrained('roberta-large')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deV4pRt2noeF"
      },
      "source": [
        "`TODO-2`: Complete the `lm_guess_sent` function to get the probability of each verbalizer in the `<mask>`, then make the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CildcznAK3h6"
      },
      "outputs": [],
      "source": [
        "def get_targets(verbalizer = 1):  #retreives the token ids for the verbalizers\n",
        "  targets = verbalize(verbalizer).keys()\n",
        "  target_ids = []\n",
        "  for target in targets:\n",
        "    id= tokenizer.get_vocab().get(\"\\u0120\"+ target, None) #how roberta ecodes wods\n",
        "    target_ids.append(id)\n",
        "  return target_ids\n",
        "\n",
        "def lm_guess_sent(model, text, template_num = 1, verb_num = 1, context_samples = None, context_labels = None):\n",
        "  if torch.cuda.is_available():\n",
        "    device = 'cuda:0'\n",
        "  else:\n",
        "    device = 'cpu'\n",
        "  model = model.to(device)\n",
        "\n",
        "  verbalizer = verbalize(verb_num) # choose a pair of verbalizers\n",
        "  target_ids = get_targets(verb_num) # get ids of verbalizers\n",
        "  text_template = template(text, template_num, context_samples=context_samples, context_labels=context_labels) # get a template with text\n",
        "\n",
        "  # TODO: encode texts with the template (text_template), return tensor\n",
        "  encoded_input = tokenizer(text_template, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "  encoded_input = encoded_input.to(device)\n",
        "\n",
        "  masked_index = torch.nonzero(encoded_input[\"input_ids\"][0] == tokenizer.mask_token_id, as_tuple=False).squeeze(-1).to(device) #getting index of mask token\n",
        "  model_outputs = model(**encoded_input)\n",
        "  outputs = model_outputs[\"logits\"]\n",
        "\n",
        "  # TODO: get the logits for masked tokens\n",
        "  logits = outputs[0, masked_index, :]\n",
        "\n",
        "  probs = logits.softmax(dim=-1) # probability of tokens\n",
        "\n",
        "  # TODO: get the probability of the two verbalizer tokens\n",
        "  probs = probs[:, target_ids]\n",
        "\n",
        "  # TODO: get prediction as the index with higher probability\n",
        "  _, predictions = torch.max(probs, dim=-1)\n",
        "  input_ids = encoded_input[\"input_ids\"][0]\n",
        "  tokens = input_ids.detach().cpu().numpy().copy()\n",
        "  p = target_ids[predictions]\n",
        "\n",
        "  prediction = verbalizer[tokenizer.decode([p]).strip()] #get corresponging label from verbalizer\n",
        "  return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO_e65cMAslb"
      },
      "source": [
        "<a name=\"prompt1\"></a>\n",
        "### 2.1 Zero-shot Prompting\n",
        "\n",
        "\n",
        "We will be using the IMDB dataset again to test prompting in the zero shot setting.\n",
        "\n",
        "We need two things to do the prompting\n",
        "\n",
        "- a **Verbalizer** that matches a word to each label\n",
        "- a **Template** to add the review, with one masked token that will predict one of the verbalizers\n",
        "\n",
        "Success of this method varies by template and verbalizer, so it is nice to test a few."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fekboRQ1d6xL"
      },
      "outputs": [],
      "source": [
        "def verbalize(num = 1):\n",
        "  if num == 1:\n",
        "    return {\"great\":1, \"horrible\":0}\n",
        "  if num == 2:\n",
        "    return {\"great\":1, \"terrible\":0}\n",
        "\n",
        "\n",
        "def template(text, num = 1, context_samples=None, context_labels=None):\n",
        "    if num == 1:\n",
        "      return \"It was <mask>.\" + text\n",
        "    if num == 2:\n",
        "      return \"So <mask>!\" + text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewv9hKEejyZP"
      },
      "source": [
        "Alright, lets see how the pre-trained roberta does on the prompted sentiment analysis.\n",
        "\n",
        "#### Verbalizer #1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WQUxhDj6Ojqo"
      },
      "outputs": [],
      "source": [
        "test_data_subset = pd.DataFrame(test_dataset[random.choices(range(len(test_dataset)), k=500)])\n",
        "\n",
        "guess = test_data_subset.apply(lambda x: lm_guess_sent(model, x['text'], template_num = 1, verb_num = 1), axis=1).tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDyc3IpDkIGR",
        "outputId": "1998b806-48a2-46b7-efcc-5cdccd97758d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy : 0.882\n",
            "Positive Accuracy : 0.9512195121951219\n",
            "Negative Accuracy : 0.8149606299212598\n",
            "F1 : 0.882\n"
          ]
        }
      ],
      "source": [
        "test_data_subset['guess'] = guess\n",
        "\n",
        "print(\"Accuracy :\", accuracy_score(test_data_subset['label'], test_data_subset['guess']))\n",
        "print(\"Positive Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==1]['label'], test_data_subset[test_data_subset['label']==1]['guess']))\n",
        "print(\"Negative Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==0]['label'], test_data_subset[test_data_subset['label']==0]['guess']))\n",
        "print(\"F1 :\", f1_score(test_data_subset['label'], test_data_subset['guess'], average = 'micro'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2PyYK7LzDRE"
      },
      "source": [
        "It seems the first verbalizer works better for the Positive reviews. How can we improve the performance without retrain or finetune the model?\n",
        "\n",
        "#### Verbalizer #2\n",
        "Lets try different verbalizers (selection 2).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Zf8yGe65y8uU"
      },
      "outputs": [],
      "source": [
        "guess = test_data_subset.apply(lambda x: lm_guess_sent(model, x['text'], template_num = 1, verb_num = 2), axis=1).tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWubfN7Cy87R",
        "outputId": "289304c9-152f-4b55-83a9-b7adb2d97752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy : 0.9\n",
            "Positive Accuracy : 0.9065040650406504\n",
            "Negative Accuracy : 0.8937007874015748\n",
            "F1 : 0.9\n"
          ]
        }
      ],
      "source": [
        "test_data_subset['guess2'] = guess\n",
        "\n",
        "print(\"Accuracy :\", accuracy_score(test_data_subset['label'], test_data_subset['guess2']))\n",
        "print(\"Positive Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==1]['label'], test_data_subset[test_data_subset['label']==1]['guess2']))\n",
        "print(\"Negative Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==0]['label'], test_data_subset[test_data_subset['label']==0]['guess2']))\n",
        "\n",
        "print(\"F1 :\", f1_score(test_data_subset['label'], test_data_subset['guess2'], average = 'micro'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hgj5mdSZvknh"
      },
      "source": [
        "**Thinking**: The second verbalizer seems work well with ~90% accuracy in both classes! Why do you think this formulation of the task works in the zero-shot setting? Can you think of any ways to *pick the most effective verbalizers* in a more systematic way?\n",
        "\n",
        "You can feel free to try your own templates/verbalizers to see how your design choices affect performance, and which ones could improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbsGhu95ux4s"
      },
      "source": [
        "<a name=\"prompt2\"></a>\n",
        "### 2.2 Few-shot Prompting\n",
        "\n",
        "Now given that we have an access to a very small labeled dataset (e.g. 5 samples), how can we make a great use of these information?\n",
        "\n",
        "If we finetune the model on these 5 samples, the model is very likely to overfit to some biased shortcuts. **Recall the prompting trick, do we have some ways to re-design the template to combine the labeled samples?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "pY3UnDeMI1D9"
      },
      "outputs": [],
      "source": [
        "train_data = load_dataset('imdb', split='train')\n",
        "train_data = train_data.shuffle(seed=42)\n",
        "fewshot_samples = train_data.select(range(10))\n",
        "\n",
        "context_samples = fewshot_samples['text']\n",
        "context_labels = fewshot_samples['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "N9Nh96XoW-t8"
      },
      "outputs": [],
      "source": [
        "def verbalize(num = 1):\n",
        "  if num == 1:\n",
        "    return {\"great\":1, \"horrible\":0}\n",
        "  if num == 2:\n",
        "    return {\"great\":1, \"terrible\":0}\n",
        "\n",
        "\n",
        "def template(text, num = 1, context_samples = None, context_labels = None):\n",
        "    if num == 1:\n",
        "      temp = \"It was <mask>.\" + text\n",
        "      pos_prefix = \"It was great.\"\n",
        "      neg_prefix = \"It was horrible.\"\n",
        "    elif num == 2:\n",
        "      temp = \"So <mask>!\" + text\n",
        "      pos_prefix = \"It was great.\"\n",
        "      neg_prefix = \"It was terrible.\"\n",
        "    else:\n",
        "      raise NotImplemented\n",
        "\n",
        "    # Build 'Context' with few-shot labeled samples\n",
        "    if context_samples is not None:\n",
        "      assert context_labels is not None, 'Please provide labels to the few-shot samples!'\n",
        "      context = ''\n",
        "      for c,y in zip(context_samples, context_labels):\n",
        "        if y==0:\n",
        "          context += (neg_prefix+' '.join(c.split(' ')[:25])+'//')\n",
        "        elif y==1:\n",
        "          context += (pos_prefix+' '.join(c.split(' ')[:25])+'//')\n",
        "      return context+temp\n",
        "    return temp\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "VpqnmIEzI8a7",
        "outputId": "597ac396-98e7-4197-979b-8ff4c06272f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'It was great.There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier//It was great.This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of//It was horrible.George P. Cosmatos\\' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn\\'t win the war in Vietnam. They caused damage to//It was great.In the process of trying to establish the audiences\\' empathy with Jake Roedel (Tobey Maguire) the filmmakers slander the North and the Jayhawkers. Missouri never//It was horrible.Yeh, I know -- you\\'re quivering with excitement. Well, *The Secret Lives of Dentists* will not upset your expectations: it\\'s solidly made but essentially unimaginative,//It was great.While this movie\\'s style isn\\'t as understated and realistic as a sound version probably would have been, this is still a very good film. In//It was great.I give this movie 7 out of 10 because the villains were interesting in their roles and the unknown batwoman creates an interesting \"guess who\"//It was horrible.really awful... lead actor did OK... the film, plot etc was completely crap and inaccurate it may as well have been a sequel to well...//It was horrible.Good grief I can\\'t even begin to describe how poor this film is. Don\\'t get me wrong, I wasn\\'t expecting much to begin with. Let\\'s//It was great.Home Room deals with a Columbine-like high-school shooting but rather than hashing over the occurrence itself the film portrays the aftermath and what happened to//It was <mask>.this film is so unbelievably awful! everything about it was rubbish. you cant say anything good about this film, the acting, script, directing, effects are all just as bad as each other. even ed wood could have done a better job than this. i seriously recommended staying away from this movie unless you want to waste about 100mins of your life or however long the film was. i forget. this is the first time i wrote a comment about a film on IMDb, but this film was just on TV and i had to let the world of movie lovers know that this film sucked balls!!!!!!!!!!!! so if you have any decency left in you. go and rent a much better bad movie like critters 3'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's see how the template would look like\n",
        "template(test_data_subset['text'][0], num = 1, context_samples = context_samples, context_labels = context_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc8rGlX4wbdX"
      },
      "source": [
        "Then we can test how the model performs with **few-shot prompting**.\n",
        "\n",
        "#### Verbalizer number 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "U1DzL_36KkDG"
      },
      "outputs": [],
      "source": [
        "guess = test_data_subset.apply(lambda x: lm_guess_sent(model, x['text'], template_num = 1, verb_num = 1, context_samples=context_samples, context_labels=context_labels), axis=1).tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MtrsFIehPko",
        "outputId": "2b2e15bc-722b-43c4-eeca-43a033d91422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy : 0.522\n",
            "Positive Accuracy : 1.0\n",
            "Negative Accuracy : 0.05905511811023622\n",
            "F1 : 0.522\n"
          ]
        }
      ],
      "source": [
        "test_data_subset['10shots-guess'] = guess\n",
        "\n",
        "print(\"Accuracy :\", accuracy_score(test_data_subset['label'], test_data_subset['10shots-guess']))\n",
        "print(\"Positive Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==1]['label'], test_data_subset[test_data_subset['label']==1]['10shots-guess']))\n",
        "print(\"Negative Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==0]['label'], test_data_subset[test_data_subset['label']==0]['10shots-guess']))\n",
        "print(\"F1 :\", f1_score(test_data_subset['label'], test_data_subset['10shots-guess'], average = 'micro'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGeQANEOY7Bu"
      },
      "source": [
        "#### Verbalizer number 2\n",
        "\n",
        "Now we will use different verbalizers to see how the model performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ULH7ghzWjZ-k"
      },
      "outputs": [],
      "source": [
        "guess = test_data_subset.apply(lambda x: lm_guess_sent(model, x['text'], template_num = 1, verb_num = 2, context_samples=context_samples, context_labels=context_labels), axis=1).tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaTn6mhAjaYd",
        "outputId": "7c245345-5a25-40d7-9dcd-019879e29bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy : 0.492\n",
            "Positive Accuracy : 1.0\n",
            "Negative Accuracy : 0.0\n",
            "F1 : 0.492\n"
          ]
        }
      ],
      "source": [
        "test_data_subset['10shots-guess2'] = guess\n",
        "\n",
        "print(\"Accuracy :\", accuracy_score(test_data_subset['label'], test_data_subset['10shots-guess2']))\n",
        "print(\"Positive Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==1]['label'], test_data_subset[test_data_subset['label']==1]['10shots-guess2']))\n",
        "print(\"Negative Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==0]['label'], test_data_subset[test_data_subset['label']==0]['10shots-guess2']))\n",
        "print(\"F1 :\", f1_score(test_data_subset['label'], test_data_subset['10shots-guess2'], average = 'micro'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN5VRZ2hlCO-"
      },
      "source": [
        "**Thinking**: What do you think of the performance? Why do you think it could happen? What can we do to improve?\n",
        "\n",
        "\n",
        "Feel free to process or change the prompts/contexts as you like, then you can see how your design choices could influence the few-shot prompting performance :)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
